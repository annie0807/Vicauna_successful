{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f9f704-ae5a-4cf4-98d9-c3be31c74f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import json\n",
    "import contextlib, unicodedata, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c74818-7b68-4d1f-a66a-6f61a7473b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f20b1e2-a923-4cf7-b4dc-d0724de2e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "5b51a88f-44f3-4bb8-8f2f-4869fa42c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/raw_text\n",
      "/Users/quert/Downloads/gcp_tmp\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/raw_text\n",
    "clusters = []\n",
    "with open(\"title_page_whole_test.txt\", \"r\") as fread:\n",
    "    for line in fread.readlines():\n",
    "        clusters.append(json.loads(line.strip()))\n",
    "%cd /Users/quert/Downloads/gcp_tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b71fab-e22b-4bc3-ba17-46a9301d7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0].keys()\n",
    "# title_non_update_link\n",
    "# title_update_link\n",
    "# update_bs4\n",
    "# non_update_bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821ed47-256e-4b82-b46d-47e0fd79b941",
   "metadata": {},
   "source": [
    "### Get the indices of required instances\n",
    "* Keep English language, with non-updated full-content existed, and meaningful updated full-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "a99b7f5f-256f-469c-851d-2ae8486682f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 232)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create idx list\n",
    "all_idx = [i for i in range(len(clusters))]\n",
    "rmved_idx = []\n",
    "\n",
    "for idx in range(len(clusters)):\n",
    "    try:\n",
    "        if bool(len(clusters[idx]['non_update_all_paragraph'].split('. ')) == 1):\n",
    "            rmved_idx.append(idx)\n",
    "    except:\n",
    "        error_idx.append(idx)\n",
    "idx_list = Diff(all_idx, rmved_idx)\n",
    "len(all_idx), len(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "1939a2d5-7574-4f8d-a5f2-5aae48ff246f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 226)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmved_idx = []\n",
    "for idx in idx_list:\n",
    "    if len(clusters[idx]['update_first_paragraph'].split())<=10:\n",
    "        rmved_idx.append(idx)\n",
    "id_list = Diff(idx_list, rmved_idx)\n",
    "len(idx_list), len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "8ae3467a-a01a-4a7a-9c95-83c9749368fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 201)"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by language of summ==English\n",
    "en_ids = []\n",
    "for idx in id_list:\n",
    "    if isEnglish(clusters[idx]['wiki_portal_summary']):\n",
    "        en_ids.append(idx)\n",
    "len(id_list), len(en_ids)\n",
    "# after filtering, we have 201 instances in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "e3badb8e-3790-4ecb-a074-799bd959e24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92052637-4d52-4f0e-a18e-84a2c0b99d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id with same section name to dataframe\n",
    "# train_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./train_sec.csv')\n",
    "# test_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./test_sec.csv')\n",
    "# val_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./val_sec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdf0fa-1c5f-4812-aeab-0d0ddcc669e7",
   "metadata": {},
   "source": [
    "### Parse the unicode into text\n",
    "* Parse the unicode into text\n",
    "* Paragraphs are separate with `. \\\\c\\\\c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f89c23-5525-4ccb-a954-ad2e0cdebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "with open(\"val_text.txt.src\", \"w\") as f:\n",
    "    # for idx in en_ids:\n",
    "    for idx in sec_ids:\n",
    "        # bef_rec = []\n",
    "        non_update_all_paragraph = clusters[idx]['non_update_all_paragraph'].split('\\n')\n",
    "        # bef_rec.append(non_update_all_paragraph)\n",
    "        s = '.\\c'\n",
    "        bef_rec_str = s.join(non_update_all_paragraph).replace(\"\\\\\\\\\\c\", \"\\c\\c\")\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            print({unicodedata.normalize('NFKD', bef_rec_str).encode('utf-8', 'ignore').decode('utf-8')})\n",
    "with open(\"val_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        log.append(line.strip().replace(\".\\\\\\\\c\", \"\\c\\c\").replace(\"\\\\\\'s\", \"'s\").replace(\"{\\'\", \"\").replace(\"\\'}\", \"\").replace(\".\\\\c\\\\c\", \". \\\\c\\\\c\"))\n",
    "with open(\"val_text.txt.src\", \"w\") as f:\n",
    "    for line in log:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd382e9d-1043-438a-82b5-3ab5db34e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "with open(\"val_text.txt.tgt\", \"w\") as f:\n",
    "    # for idx in en_ids:\n",
    "    for idx in sec_ids:\n",
    "        # tgt_rec = []\n",
    "        update_all_paragraph = clusters[idx][\"update_all_paragraph\"].split(\"\\n\")\n",
    "        # tgt_rec.extend(update_all_paragraph)\n",
    "        s = \".\\c\"\n",
    "        tgt_rec_str = s.join(update_all_paragraph).replace(\"\\\\\\\\\\c\", \"\\c\\c\")\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            print({unicodedata.normalize('NFKD', tgt_rec_str).encode('utf-8', 'ignore').decode('utf-8')})\n",
    "with open(\"val_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        log.append(line.strip().replace(\".\\\\\\\\c\", \"\\c\\c\").replace(\"\\\\\\'s\", \"'s\").replace(\"{\\'\", \"\").replace(\"\\'}\", \"\").replace(\".\\\\c\\\\c\", \". \\\\c\\\\c\"))\n",
    "with open(\"val_text.txt.tgt\", \"w\") as f:\n",
    "    for line in log:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e1b07-e3cc-4a6d-9f8b-9a483cd30ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text (src, tgt) to pt format\n",
    "srcs, tgts = [], []\n",
    "with open(\"val_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        srcs.append(line.strip())\n",
    "with open(\"val_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tgts.append(line.strip())\n",
    "\n",
    "wrap = []\n",
    "for idx in range(len(srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = srcs[idx]\n",
    "    idx_content['summary'] = tgts[idx].lstrip('{\"').lstrip(\"{'\").rstrip('\\n')\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_val.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180b606-e927-476d-9da0-7784458e0814",
   "metadata": {},
   "source": [
    "### Extract the section name and number of paragraphs under each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "9146defc-93f5-48f4-b83a-b789b1dcb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Old_pars_num(clu_idx):\n",
    "    nonupdated_raw_text = clusters[clu_idx][\"non_update_bs4\"]\n",
    "    # end_idx = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', raw_text).index(\"References\")\n",
    "    # sec_names = re.findall('<span class=\"toctext\">(.*)</span>', raw_text)[:end_idx]\n",
    "    \n",
    "    old_main_secs = re.findall('<span class=\"tocnumber\">\\d+</span> <span class=\"toctext\">(.*)</span>', nonupdated_raw_text)\n",
    "    old_all_secs = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', nonupdated_raw_text)\n",
    "    \n",
    "    # Return the global index of main section\n",
    "    old_main_global_ids = []\n",
    "    for old_main_sec in old_main_secs:\n",
    "        # The indices of main sec in all_secs\n",
    "        ids = old_all_secs.index(old_main_sec)\n",
    "        old_main_global_ids.append(ids)\n",
    "    \n",
    "    # Concatenate the parent section name with sub-section name\n",
    "    # Some page only include summary without other sections\n",
    "    old_mod_secs = old_all_secs.copy()\n",
    "    try:\n",
    "        for idx in range(len(old_mod_secs)):\n",
    "            if idx in old_main_global_ids: old_mod_secs[idx]==old_mod_secs[idx]\n",
    "            else: \n",
    "                old_bool_lst = list(np.asarray(old_main_global_ids)<idx)\n",
    "                old_parent_ord = [str(val) for val in old_bool_lst].count(\"True\")\n",
    "                old_parent_idx = old_main_global_ids[old_parent_ord-1]\n",
    "                old_mod_secs[idx] = str(old_mod_secs[old_parent_idx]) + \" - \" + old_all_secs[idx]\n",
    "        \n",
    "        # Get the section names except \"References\"\n",
    "        old_end_idx = old_mod_secs.index(\"References\")\n",
    "        old_sec_names = old_mod_secs[:old_end_idx]\n",
    "        \n",
    "        # Count the #paragraphs for each section (main section and sub-section), exclude the section with zero paragraph \n",
    "        old_num_pars = []\n",
    "        for i in range(len(old_sec_names)):\n",
    "            old_pars_str = str(nonupdated_raw_text.split('<span class=\"mw-headline')[i+1])\n",
    "            old_num_par = len(re.split(\"<p>|<li>\", old_pars_str)[1:])\n",
    "            # old_num_par = len(re.split(\"<p>\", old_pars_str)[1:])\n",
    "            old_num_pars.append(old_num_par)\n",
    "        old_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        old_num_pars.insert(0, old_num_summ)\n",
    "        old_sec_names.insert(0, \"Summary\")\n",
    "        old_sec_pars = dict(zip(old_sec_names, old_num_pars))\n",
    "        old_secs_pars_nums = {k: v for k, v in old_sec_pars.items() if v!=0}\n",
    "        if \"Notes\" in old_secs_pars_nums.keys(): del old_secs_pars_nums[\"Notes\"]\n",
    "        elif \"See also\" in old_secs_pars_nums.keys(): del old_secs_pars_nums[\"See also\"]\n",
    "        \n",
    "    except: # page with only \"Summary\" exists\n",
    "        old_sec_names, old_num_pars = [], []\n",
    "        old_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        old_sec_names.append(\"Summary\")\n",
    "        old_num_pars.append(old_num_summ) \n",
    "        old_sec_pars = dict(zip(old_sec_names, old_num_pars))\n",
    "        old_secs_pars_nums = {k: v for k, v in old_sec_pars.items() if v!=0}\n",
    "        \n",
    "    # return list(old_secs_pars_nums)\n",
    "    return list(old_secs_pars_nums.keys())\n",
    "    # return old_secs_pars_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "8c31b26c-2ae7-4afb-9ba8-3236b3768e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_pars_num(clu_idx):\n",
    "    updated_raw_text = clusters[clu_idx][\"update_bs4\"] \n",
    "    \n",
    "    new_main_secs = re.findall('<span class=\"tocnumber\">\\d+</span> <span class=\"toctext\">(.*)</span>', updated_raw_text)\n",
    "    new_all_secs = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', updated_raw_text)\n",
    "    \n",
    "    # Return the global index of main section\n",
    "    new_main_global_ids = []\n",
    "    for new_main_sec in new_main_secs:\n",
    "        # The indices of main sec in all_secs\n",
    "        ids = new_all_secs.index(new_main_sec)\n",
    "        new_main_global_ids.append(ids)\n",
    "    \n",
    "    # Concatenate the parent section name with sub-section name\n",
    "    new_mod_secs = new_all_secs.copy()\n",
    "    try:\n",
    "        for idx in range(len(new_mod_secs)):\n",
    "            if idx in new_main_global_ids: new_mod_secs[idx]==new_mod_secs[idx]\n",
    "            else: \n",
    "                new_bool_lst = list(np.asarray(new_main_global_ids)<idx)\n",
    "                new_parent_ord = [str(val) for val in new_bool_lst].count(\"True\")\n",
    "                new_parent_idx = new_main_global_ids[new_parent_ord-1]\n",
    "                new_mod_secs[idx] = str(new_mod_secs[new_parent_idx]) + \" - \" + new_all_secs[idx]\n",
    "        \n",
    "        # Get the section names except \"References\"\n",
    "        new_end_idx = new_mod_secs.index(\"References\")\n",
    "        new_sec_names = new_mod_secs[:new_end_idx]\n",
    "        \n",
    "        # Count the #paragraphs under each section (main section and sub-section), exclude the section with zero paragraph \n",
    "        new_num_pars = []\n",
    "        for i in range(len(new_sec_names)):\n",
    "            new_pars_str = str(clusters[clu_idx][\"update_bs4\"].split('<span class=\"mw-headline')[i+1])\n",
    "            new_num_par = len(re.split(\"<p>|<li>\", new_pars_str)[1:])\n",
    "            # new_num_par = len(re.split(\"<p>\", new_pars_str)[1:])\n",
    "            new_num_pars.append(new_num_par)\n",
    "        new_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        new_num_pars.insert(0, new_num_summ)\n",
    "        new_sec_names.insert(0, \"Summary\")\n",
    "        new_sec_pars = dict(zip(new_sec_names, new_num_pars))\n",
    "        new_secs_pars_nums = {k: v for k, v in new_sec_pars.items() if v!=0}\n",
    "        \n",
    "        if \"Notes\" in new_secs_pars_nums.keys(): del new_secs_pars_nums[\"Notes\"]\n",
    "        elif \"See also\" in new_secs_pars_nums.keys(): del new_secs_pars_nums[\"See also\"]\n",
    "        \n",
    "    except:\n",
    "        new_sec_names, new_num_pars = [], []\n",
    "        new_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        new_sec_names.append(\"Summary\")\n",
    "        new_num_pars.append(new_num_summ) \n",
    "        new_sec_pars = dict(zip(new_sec_names, new_num_pars))\n",
    "        new_secs_pars_nums = {k: v for k, v in new_sec_pars.items() if v!=0}\n",
    "        \n",
    "    # return list(new_secs_pars_nums)\n",
    "    return list(new_secs_pars_nums.keys())\n",
    "    # return new_secs_pars_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "a3b08f8a-aa6b-4adb-a7d0-e41cc7916981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the instances with same sections\n",
    "sec_ids = []\n",
    "for idx in en_ids:\n",
    "    try:\n",
    "        if Old_pars_num(idx)==New_pars_num(idx):\n",
    "            sec_ids.append(idx)\n",
    "    except:\n",
    "        pass\n",
    "# -- Train set\n",
    "# We have 911 instances with same #sections and #paragraphs\n",
    "# We have 1334 instances with same #sections -> 423 instances with same #sections and different #paragraphs\n",
    "# Apply the sec_ids to extract and construct new datset\n",
    "# -- Test set\n",
    "# We have 168 instances with same #sections and #paragraphs\n",
    "# We have 168 instances with same #sections -> 0 instances with same #sections and different #paragraphs\n",
    "# -- Val set\n",
    "# We have instances with same #sections and #paragraphs\n",
    "# We have instances with same #sections -> instances with same #sections and different #paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "804632a6-7aa9-497c-b0e8-757eb61bbdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec692dea-fda5-443c-be61-4669faca3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our unsized data\n",
    "%cd /Users/quert/Downloads/gcp_tmp\n",
    "srcs, tgts = [], []\n",
    "test_pt = torch.load(\"./ptfile/same_secs_old/netku_samesecs_train.pt\")\n",
    "for idx in range(len(test_pt)):\n",
    "    srcs.append(test_pt[idx][\"document\"])\n",
    "    tgts.append(test_pt[idx][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81af69-1b01-43a4-b186-5c765a78bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the paragraphs\n",
    "new_srcs, new_tgts = [], []\n",
    "for idx in range(len(srcs)):\n",
    "    pars = srcs[idx].split(\"\\\\c\\\\c\")\n",
    "    nums = np.sum(list(Old_pars_num(sec_ids[idx]).values()))\n",
    "    new_src = \"\\\\c\\\\c\".join(pars[:nums])\n",
    "    new_srcs.append(new_src)\n",
    "for idx in range(len(tgts)):\n",
    "    pars = tgts[idx].split(\"\\\\c\\\\c\")\n",
    "    nums = np.sum(list(New_pars_num(sec_ids[idx]).values()))\n",
    "    new_tgt = \"\\\\c\\\\c\".join(pars[:nums])\n",
    "    new_tgts.append(new_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8fe7a-95ac-4ef7-bf48-11b3a3a183e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create re-sized data\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_new\n",
    "with open(\"train_text.txt.src\", \"w\") as f:\n",
    "    for new_src in new_srcs:\n",
    "        f.write(new_src+\"\\n\")\n",
    "with open(\"train_text.txt.tgt\", \"w\") as f:\n",
    "    for new_tgt in new_tgts:\n",
    "        f.write(new_tgt+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849d86b-2b78-4a51-aebc-77471fb56af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert src, tgt to pt format\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_new\n",
    "wrap = []\n",
    "for idx in range(len(new_srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = new_srcs[idx]\n",
    "    idx_content['summary'] = new_tgts[idx]\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff562b-3803-4423-9596-014d2496ca87",
   "metadata": {},
   "source": [
    "### Insert section names to each paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289ba9c-d82b-4e63-b631-e096e7d27217",
   "metadata": {},
   "source": [
    "* Check if the #paragraphs from instances equals to our counts\n",
    "* `. \\\\c\\\\c` -> `. <Timeline> \\\\c\\\\c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "711eef6c-7b86-4ba2-b200-97145816df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the numbers of each section name (dicts) into (list) \n",
    "old_list_secnums = [Old_pars_num(idx) for idx in sec_ids]\n",
    "new_list_secnums = [New_pars_num(idx) for idx in sec_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "32c92594-9d71-4e0d-922c-2792e79fcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the labeled data\n",
    "srcs, tgts = [], []\n",
    "test_pt = torch.load(\"./ptfile/same_secs_new/netku_samesecs_train.pt\")\n",
    "for idx in range(len(test_pt)):\n",
    "    srcs.append(test_pt[idx][\"document\"])\n",
    "    tgts.append(test_pt[idx][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf269d33-fc4f-4ee9-b2fc-598d6209ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(list(old_list_secnums[0].values())), len(srcs[0].split(\"\\\\c\\\\c\"))\n",
    "# new_srcs = []\n",
    "'''\n",
    "nums = list(old_list_secnums[0].values())\n",
    "titles = list(old_list_secnums[0].keys())\n",
    "src_article = srcs[0]\n",
    "paragraphs = src_article.split(\"\\\\c\\\\c\")\n",
    "paragraphs[0] = paragraphs[0]+title[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "b04521fa-5ee9-4716-8c5e-443865e13bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert counts into indices: (index: title)\n",
    "titles_keys = []\n",
    "for idx in range(len(new_list_secnums)):\n",
    "    titles = list(old_list_secnums[idx].keys())\n",
    "    titles_as_keys = []\n",
    "    for i in range(len(titles)):\n",
    "        for _ in range(list(new_list_secnums[idx].values())[i]):\n",
    "            titles_as_keys.append(titles[i])\n",
    "    titles_keys.append(titles_as_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "bf9e0640-94e2-491f-a48a-7f1db2410b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in range(len(srcs)):\n",
    "added_secs = []\n",
    "for idx in range(len(titles_keys)):\n",
    "    splitted_pars = tgts[idx].split(\"\\\\c\\\\c\")\n",
    "    pars_in = []\n",
    "    for i in range(len(splitted_pars)):\n",
    "        par_with_secname = splitted_pars[i] + \" <\" + titles_keys[idx][i] + \">\" + \" \\\\c\\\\c\"\n",
    "        par_with_secname = par_with_secname.replace(\"  <\", \" <\")\n",
    "        pars_in.append(par_with_secname)\n",
    "    pars = \"\".join(pars_in)\n",
    "    added_secs.append(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "83710fb1-2b89-438f-95fb-f18567ec392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert\n"
     ]
    }
   ],
   "source": [
    "# Convert our new-construct data into src, tgt\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/ \n",
    "with open(\"train_text.txt.tgt\", \"w\") as f:\n",
    "    for instance in added_secs:\n",
    "        f.write(instance+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "5d81f34b-2559-4b0b-88c6-52c6e052469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert\n"
     ]
    }
   ],
   "source": [
    "# Convert src, tgt into single pt format\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/\n",
    "srcs, tgts = [], []\n",
    "with open(\"train_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        srcs.append(line.strip())\n",
    "with open(\"train_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tgts.append(line.strip())\n",
    "\n",
    "wrap = []\n",
    "for idx in range(len(srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = srcs[idx]\n",
    "    idx_content['summary'] = tgts[idx]\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fe501-95dd-4a62-b3cd-4b012ede279d",
   "metadata": {},
   "source": [
    "### Trigger Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "389fd7fc-08dd-4d3a-bdc1-223f7e623309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/\n",
    "triggers = [clusters[idx][\"wiki_portal_summary\"] for idx in sec_ids]\n",
    "with open(\"samesecs_triggers_test.txt\", \"w\") as f:\n",
    "    for trigger in triggers:\n",
    "        f.write(trigger+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89036952-3f0c-4d2b-b12b-4b7e6c48838b",
   "metadata": {},
   "source": [
    "* Since full-article includes \"summary\", we have to remove the \"summary\" before alignment -> then add back to the text afterwards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5c2adc-907a-4f64-9056-8682ab1b784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary\n",
    "# summary = clusters[2][\"non_update_wiki_summary\"]\n",
    "contents_with_summ = []\n",
    "with open(\"test_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        contents_with_summ.append(line)\n",
    "\n",
    "for idx in range(len(en_ids)):\n",
    "    real_id = en_ids[idx]\n",
    "    s = \"\\\\c\\\\c\"\n",
    "    summs = clusters[real_id][\"non_update_wiki_summary\"].split(\"\\n\") # splitted summary\n",
    "    full_content_with_summ = contents_with_summ[idx]\n",
    "    with_summs = full_content_with_summ.split(\"\\\\c\\\\c\") # splitted contents with summ\n",
    "    for par in with_summs:\n",
    "        if par in summs: with_summs.remove(par) \n",
    "with open(\"test_no_summs.txt.src\", \"w\") as f:\n",
    "    for line in with_summs: f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07639809-303f-4fbe-9506-5d369b31db95",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_summs = []\n",
    "with open(\"test_no_summs.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines(): no_summs.append(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
