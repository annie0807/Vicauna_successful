{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "55f9f704-ae5a-4cf4-98d9-c3be31c74f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import json\n",
    "import contextlib, unicodedata, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import difflib\n",
    "from difflib import SequenceMatcher\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "67c74818-7b68-4d1f-a66a-6f61a7473b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Diff(li1, li2):\n",
    "    li_dif = [i for i in li1 + li2 if i not in li1 or i not in li2]\n",
    "    return li_dif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "7f20b1e2-a923-4cf7-b4dc-d0724de2e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "id": "5b51a88f-44f3-4bb8-8f2f-4869fa42c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/raw_text\n",
      "/Users/quert/Downloads/gcp_tmp\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/raw_text\n",
    "clusters = []\n",
    "with open(\"title_page_whole_test.txt\", \"r\") as fread:\n",
    "    for line in fread.readlines():\n",
    "        clusters.append(json.loads(line.strip()))\n",
    "%cd /Users/quert/Downloads/gcp_tmp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b71fab-e22b-4bc3-ba17-46a9301d7499",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters[0].keys()\n",
    "# title_non_update_link\n",
    "# title_update_link\n",
    "# update_bs4\n",
    "# non_update_bs4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0821ed47-256e-4b82-b46d-47e0fd79b941",
   "metadata": {},
   "source": [
    "### Get the indices of required instances\n",
    "* Keep English language, with non-updated full-content existed, and meaningful updated full-content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "id": "a99b7f5f-256f-469c-851d-2ae8486682f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(239, 232)"
      ]
     },
     "execution_count": 648,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create idx list\n",
    "all_idx = [i for i in range(len(clusters))]\n",
    "rmved_idx = []\n",
    "\n",
    "for idx in range(len(clusters)):\n",
    "    try:\n",
    "        if bool(len(clusters[idx]['non_update_all_paragraph'].split('. ')) == 1):\n",
    "            rmved_idx.append(idx)\n",
    "    except:\n",
    "        error_idx.append(idx)\n",
    "idx_list = Diff(all_idx, rmved_idx)\n",
    "len(all_idx), len(idx_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "id": "1939a2d5-7574-4f8d-a5f2-5aae48ff246f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(232, 226)"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmved_idx = []\n",
    "for idx in idx_list:\n",
    "    if len(clusters[idx]['update_first_paragraph'].split())<=10:\n",
    "        rmved_idx.append(idx)\n",
    "id_list = Diff(idx_list, rmved_idx)\n",
    "len(idx_list), len(id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "id": "8ae3467a-a01a-4a7a-9c95-83c9749368fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(226, 201)"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter by language of summ==English\n",
    "en_ids = []\n",
    "for idx in id_list:\n",
    "    if isEnglish(clusters[idx]['wiki_portal_summary']):\n",
    "        en_ids.append(idx)\n",
    "len(id_list), len(en_ids)\n",
    "# after filtering, we have 201 instances in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "id": "e3badb8e-3790-4ecb-a074-799bd959e24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92052637-4d52-4f0e-a18e-84a2c0b99d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert id with same section name to dataframe\n",
    "# train_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./train_sec.csv')\n",
    "# test_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./test_sec.csv')\n",
    "# val_sec_df = pd.DataFrame({'Global ID': sec_ids}).to_csv('./val_sec.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bdf0fa-1c5f-4812-aeab-0d0ddcc669e7",
   "metadata": {},
   "source": [
    "### Parse the unicode into text\n",
    "* Parse the unicode into text\n",
    "* Paragraphs are separate with `. \\\\c\\\\c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1f89c23-5525-4ccb-a954-ad2e0cdebd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "with open(\"val_text.txt.src\", \"w\") as f:\n",
    "    # for idx in en_ids:\n",
    "    for idx in sec_ids:\n",
    "        # bef_rec = []\n",
    "        non_update_all_paragraph = clusters[idx]['non_update_all_paragraph'].split('\\n')\n",
    "        # bef_rec.append(non_update_all_paragraph)\n",
    "        s = '.\\c'\n",
    "        bef_rec_str = s.join(non_update_all_paragraph).replace(\"\\\\\\\\\\c\", \"\\c\\c\")\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            print({unicodedata.normalize('NFKD', bef_rec_str).encode('utf-8', 'ignore').decode('utf-8')})\n",
    "with open(\"val_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        log.append(line.strip().replace(\".\\\\\\\\c\", \"\\c\\c\").replace(\"\\\\\\'s\", \"'s\").replace(\"{\\'\", \"\").replace(\"\\'}\", \"\").replace(\".\\\\c\\\\c\", \". \\\\c\\\\c\"))\n",
    "with open(\"val_text.txt.src\", \"w\") as f:\n",
    "    for line in log:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd382e9d-1043-438a-82b5-3ab5db34e001",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = []\n",
    "with open(\"val_text.txt.tgt\", \"w\") as f:\n",
    "    # for idx in en_ids:\n",
    "    for idx in sec_ids:\n",
    "        # tgt_rec = []\n",
    "        update_all_paragraph = clusters[idx][\"update_all_paragraph\"].split(\"\\n\")\n",
    "        # tgt_rec.extend(update_all_paragraph)\n",
    "        s = \".\\c\"\n",
    "        tgt_rec_str = s.join(update_all_paragraph).replace(\"\\\\\\\\\\c\", \"\\c\\c\")\n",
    "        with contextlib.redirect_stdout(f):\n",
    "            print({unicodedata.normalize('NFKD', tgt_rec_str).encode('utf-8', 'ignore').decode('utf-8')})\n",
    "with open(\"val_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        log.append(line.strip().replace(\".\\\\\\\\c\", \"\\c\\c\").replace(\"\\\\\\'s\", \"'s\").replace(\"{\\'\", \"\").replace(\"\\'}\", \"\").replace(\".\\\\c\\\\c\", \". \\\\c\\\\c\"))\n",
    "with open(\"val_text.txt.tgt\", \"w\") as f:\n",
    "    for line in log:\n",
    "        f.write(line+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc1e1b07-e3cc-4a6d-9f8b-9a483cd30ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the text (src, tgt) to pt format\n",
    "srcs, tgts = [], []\n",
    "with open(\"val_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        srcs.append(line.strip())\n",
    "with open(\"val_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tgts.append(line.strip())\n",
    "\n",
    "wrap = []\n",
    "for idx in range(len(srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = srcs[idx]\n",
    "    idx_content['summary'] = tgts[idx].lstrip('{\"').lstrip(\"{'\").rstrip('\\n')\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_val.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7180b606-e927-476d-9da0-7784458e0814",
   "metadata": {},
   "source": [
    "### Extract the section name and number of paragraphs under each section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "9146defc-93f5-48f4-b83a-b789b1dcb087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Old_pars_num(clu_idx):\n",
    "    nonupdated_raw_text = clusters[clu_idx][\"non_update_bs4\"]\n",
    "    # end_idx = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', raw_text).index(\"References\")\n",
    "    # sec_names = re.findall('<span class=\"toctext\">(.*)</span>', raw_text)[:end_idx]\n",
    "    \n",
    "    old_main_secs = re.findall('<span class=\"tocnumber\">\\d+</span> <span class=\"toctext\">(.*)</span>', nonupdated_raw_text)\n",
    "    old_all_secs = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', nonupdated_raw_text)\n",
    "    \n",
    "    # Return the global index of main section\n",
    "    old_main_global_ids = []\n",
    "    for old_main_sec in old_main_secs:\n",
    "        # The indices of main sec in all_secs\n",
    "        ids = old_all_secs.index(old_main_sec)\n",
    "        old_main_global_ids.append(ids)\n",
    "    \n",
    "    # Concatenate the parent section name with sub-section name\n",
    "    # Some page only include summary without other sections\n",
    "    old_mod_secs = old_all_secs.copy()\n",
    "    try:\n",
    "        for idx in range(len(old_mod_secs)):\n",
    "            if idx in old_main_global_ids: old_mod_secs[idx]==old_mod_secs[idx]\n",
    "            else: \n",
    "                old_bool_lst = list(np.asarray(old_main_global_ids)<idx)\n",
    "                old_parent_ord = [str(val) for val in old_bool_lst].count(\"True\")\n",
    "                old_parent_idx = old_main_global_ids[old_parent_ord-1]\n",
    "                old_mod_secs[idx] = str(old_mod_secs[old_parent_idx]) + \" - \" + old_all_secs[idx]\n",
    "        \n",
    "        # Get the section names except \"References\"\n",
    "        old_end_idx = old_mod_secs.index(\"References\")\n",
    "        old_sec_names = old_mod_secs[:old_end_idx]\n",
    "        \n",
    "        # Count the #paragraphs for each section (main section and sub-section), exclude the section with zero paragraph \n",
    "        old_num_pars = []\n",
    "        for i in range(len(old_sec_names)):\n",
    "            old_pars_str = str(nonupdated_raw_text.split('<span class=\"mw-headline')[i+1])\n",
    "            old_num_par = len(re.split(\"<p>|<li>\", old_pars_str)[1:])\n",
    "            # old_num_par = len(re.split(\"<p>\", old_pars_str)[1:])\n",
    "            old_num_pars.append(old_num_par)\n",
    "        old_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        old_num_pars.insert(0, old_num_summ)\n",
    "        old_sec_names.insert(0, \"Summary\")\n",
    "        old_sec_pars = dict(zip(old_sec_names, old_num_pars))\n",
    "        old_secs_pars_nums = {k: v for k, v in old_sec_pars.items() if v!=0}\n",
    "        if \"Notes\" in old_secs_pars_nums.keys(): del old_secs_pars_nums[\"Notes\"]\n",
    "        elif \"See also\" in old_secs_pars_nums.keys(): del old_secs_pars_nums[\"See also\"]\n",
    "        \n",
    "    except: # page with only \"Summary\" exists\n",
    "        old_sec_names, old_num_pars = [], []\n",
    "        old_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        old_sec_names.append(\"Summary\")\n",
    "        old_num_pars.append(old_num_summ) \n",
    "        old_sec_pars = dict(zip(old_sec_names, old_num_pars))\n",
    "        old_secs_pars_nums = {k: v for k, v in old_sec_pars.items() if v!=0}\n",
    "        \n",
    "    # return list(old_secs_pars_nums)\n",
    "    return list(old_secs_pars_nums.keys())\n",
    "    # return old_secs_pars_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "8c31b26c-2ae7-4afb-9ba8-3236b3768e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def New_pars_num(clu_idx):\n",
    "    updated_raw_text = clusters[clu_idx][\"update_bs4\"] \n",
    "    \n",
    "    new_main_secs = re.findall('<span class=\"tocnumber\">\\d+</span> <span class=\"toctext\">(.*)</span>', updated_raw_text)\n",
    "    new_all_secs = re.findall('<span class=\"mw-headline\" id=.+\">(.*)</span>', updated_raw_text)\n",
    "    \n",
    "    # Return the global index of main section\n",
    "    new_main_global_ids = []\n",
    "    for new_main_sec in new_main_secs:\n",
    "        # The indices of main sec in all_secs\n",
    "        ids = new_all_secs.index(new_main_sec)\n",
    "        new_main_global_ids.append(ids)\n",
    "    \n",
    "    # Concatenate the parent section name with sub-section name\n",
    "    new_mod_secs = new_all_secs.copy()\n",
    "    try:\n",
    "        for idx in range(len(new_mod_secs)):\n",
    "            if idx in new_main_global_ids: new_mod_secs[idx]==new_mod_secs[idx]\n",
    "            else: \n",
    "                new_bool_lst = list(np.asarray(new_main_global_ids)<idx)\n",
    "                new_parent_ord = [str(val) for val in new_bool_lst].count(\"True\")\n",
    "                new_parent_idx = new_main_global_ids[new_parent_ord-1]\n",
    "                new_mod_secs[idx] = str(new_mod_secs[new_parent_idx]) + \" - \" + new_all_secs[idx]\n",
    "        \n",
    "        # Get the section names except \"References\"\n",
    "        new_end_idx = new_mod_secs.index(\"References\")\n",
    "        new_sec_names = new_mod_secs[:new_end_idx]\n",
    "        \n",
    "        # Count the #paragraphs under each section (main section and sub-section), exclude the section with zero paragraph \n",
    "        new_num_pars = []\n",
    "        for i in range(len(new_sec_names)):\n",
    "            new_pars_str = str(clusters[clu_idx][\"update_bs4\"].split('<span class=\"mw-headline')[i+1])\n",
    "            new_num_par = len(re.split(\"<p>|<li>\", new_pars_str)[1:])\n",
    "            # new_num_par = len(re.split(\"<p>\", new_pars_str)[1:])\n",
    "            new_num_pars.append(new_num_par)\n",
    "        new_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        new_num_pars.insert(0, new_num_summ)\n",
    "        new_sec_names.insert(0, \"Summary\")\n",
    "        new_sec_pars = dict(zip(new_sec_names, new_num_pars))\n",
    "        new_secs_pars_nums = {k: v for k, v in new_sec_pars.items() if v!=0}\n",
    "        \n",
    "        if \"Notes\" in new_secs_pars_nums.keys(): del new_secs_pars_nums[\"Notes\"]\n",
    "        elif \"See also\" in new_secs_pars_nums.keys(): del new_secs_pars_nums[\"See also\"]\n",
    "        \n",
    "    except:\n",
    "        new_sec_names, new_num_pars = [], []\n",
    "        new_num_summ = len(clusters[clu_idx][\"non_update_wiki_summary\"].split(\"\\n\"))\n",
    "        new_sec_names.append(\"Summary\")\n",
    "        new_num_pars.append(new_num_summ) \n",
    "        new_sec_pars = dict(zip(new_sec_names, new_num_pars))\n",
    "        new_secs_pars_nums = {k: v for k, v in new_sec_pars.items() if v!=0}\n",
    "        \n",
    "    # return list(new_secs_pars_nums)\n",
    "    return list(new_secs_pars_nums.keys())\n",
    "    # return new_secs_pars_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "id": "a3b08f8a-aa6b-4adb-a7d0-e41cc7916981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the instances with same sections\n",
    "sec_ids = []\n",
    "for idx in en_ids:\n",
    "    try:\n",
    "        if Old_pars_num(idx)==New_pars_num(idx):\n",
    "            sec_ids.append(idx)\n",
    "    except:\n",
    "        pass\n",
    "# -- Train set\n",
    "# We have 911 instances with same #sections and #paragraphs\n",
    "# We have 1334 instances with same #sections -> 423 instances with same #sections and different #paragraphs\n",
    "# Apply the sec_ids to extract and construct new datset\n",
    "# -- Test set\n",
    "# We have 168 instances with same #sections and #paragraphs\n",
    "# We have 168 instances with same #sections -> 0 instances with same #sections and different #paragraphs\n",
    "# -- Val set\n",
    "# We have instances with same #sections and #paragraphs\n",
    "# We have instances with same #sections -> instances with same #sections and different #paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "id": "804632a6-7aa9-497c-b0e8-757eb61bbdaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 652,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sec_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec692dea-fda5-443c-be61-4669faca3f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our unsized data\n",
    "%cd /Users/quert/Downloads/gcp_tmp\n",
    "srcs, tgts = [], []\n",
    "test_pt = torch.load(\"./ptfile/same_secs_old/netku_samesecs_train.pt\")\n",
    "for idx in range(len(test_pt)):\n",
    "    srcs.append(test_pt[idx][\"document\"])\n",
    "    tgts.append(test_pt[idx][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81af69-1b01-43a4-b186-5c765a78bb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize the paragraphs\n",
    "new_srcs, new_tgts = [], []\n",
    "for idx in range(len(srcs)):\n",
    "    pars = srcs[idx].split(\"\\\\c\\\\c\")\n",
    "    nums = np.sum(list(Old_pars_num(sec_ids[idx]).values()))\n",
    "    new_src = \"\\\\c\\\\c\".join(pars[:nums])\n",
    "    new_srcs.append(new_src)\n",
    "for idx in range(len(tgts)):\n",
    "    pars = tgts[idx].split(\"\\\\c\\\\c\")\n",
    "    nums = np.sum(list(New_pars_num(sec_ids[idx]).values()))\n",
    "    new_tgt = \"\\\\c\\\\c\".join(pars[:nums])\n",
    "    new_tgts.append(new_tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c8fe7a-95ac-4ef7-bf48-11b3a3a183e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create re-sized data\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_new\n",
    "with open(\"train_text.txt.src\", \"w\") as f:\n",
    "    for new_src in new_srcs:\n",
    "        f.write(new_src+\"\\n\")\n",
    "with open(\"train_text.txt.tgt\", \"w\") as f:\n",
    "    for new_tgt in new_tgts:\n",
    "        f.write(new_tgt+\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1849d86b-2b78-4a51-aebc-77471fb56af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert src, tgt to pt format\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_new\n",
    "wrap = []\n",
    "for idx in range(len(new_srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = new_srcs[idx]\n",
    "    idx_content['summary'] = new_tgts[idx]\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efff562b-3803-4423-9596-014d2496ca87",
   "metadata": {},
   "source": [
    "### Insert section names to each paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d289ba9c-d82b-4e63-b631-e096e7d27217",
   "metadata": {},
   "source": [
    "* Check if the #paragraphs from instances equals to our counts\n",
    "* `. \\\\c\\\\c` -> `. <Timeline> \\\\c\\\\c`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "id": "711eef6c-7b86-4ba2-b200-97145816df71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the numbers of each section name (dicts) into (list) \n",
    "old_list_secnums = [Old_pars_num(idx) for idx in sec_ids]\n",
    "new_list_secnums = [New_pars_num(idx) for idx in sec_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "id": "32c92594-9d71-4e0d-922c-2792e79fcac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the labeled data\n",
    "srcs, tgts = [], []\n",
    "test_pt = torch.load(\"./ptfile/same_secs_new/netku_samesecs_train.pt\")\n",
    "for idx in range(len(test_pt)):\n",
    "    srcs.append(test_pt[idx][\"document\"])\n",
    "    tgts.append(test_pt[idx][\"summary\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf269d33-fc4f-4ee9-b2fc-598d6209ae1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.sum(list(old_list_secnums[0].values())), len(srcs[0].split(\"\\\\c\\\\c\"))\n",
    "# new_srcs = []\n",
    "'''\n",
    "nums = list(old_list_secnums[0].values())\n",
    "titles = list(old_list_secnums[0].keys())\n",
    "src_article = srcs[0]\n",
    "paragraphs = src_article.split(\"\\\\c\\\\c\")\n",
    "paragraphs[0] = paragraphs[0]+title[0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "b04521fa-5ee9-4716-8c5e-443865e13bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert counts into indices: (index: title)\n",
    "titles_keys = []\n",
    "for idx in range(len(new_list_secnums)):\n",
    "    titles = list(old_list_secnums[idx].keys())\n",
    "    titles_as_keys = []\n",
    "    for i in range(len(titles)):\n",
    "        for _ in range(list(new_list_secnums[idx].values())[i]):\n",
    "            titles_as_keys.append(titles[i])\n",
    "    titles_keys.append(titles_as_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "bf9e0640-94e2-491f-a48a-7f1db2410b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add section names to tail for each paragraph\n",
    "added_secs = []\n",
    "for idx in range(len(titles_keys)):\n",
    "    splitted_pars = tgts[idx].split(\"\\\\c\\\\c\")\n",
    "    pars_in = []\n",
    "    for i in range(len(splitted_pars)):\n",
    "        par_with_secname = splitted_pars[i] + \" <\" + titles_keys[idx][i] + \">\" + \" \\\\c\\\\c\"\n",
    "        par_with_secname = par_with_secname.replace(\"  <\", \" <\")\n",
    "        pars_in.append(par_with_secname)\n",
    "    pars = \"\".join(pars_in)\n",
    "    added_secs.append(pars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "83710fb1-2b89-438f-95fb-f18567ec392e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert\n"
     ]
    }
   ],
   "source": [
    "# Convert our new-construct data into src, tgt\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/ \n",
    "with open(\"train_text.txt.tgt\", \"w\") as f:\n",
    "    for instance in added_secs:\n",
    "        f.write(instance+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 625,
   "id": "5d81f34b-2559-4b0b-88c6-52c6e052469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert\n"
     ]
    }
   ],
   "source": [
    "# Convert src, tgt into single pt format\n",
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/\n",
    "srcs, tgts = [], []\n",
    "with open(\"train_text.txt.src\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        srcs.append(line.strip())\n",
    "with open(\"train_text.txt.tgt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        tgts.append(line.strip())\n",
    "\n",
    "wrap = []\n",
    "for idx in range(len(srcs)):\n",
    "    idx_content = {}\n",
    "    idx_content['document'] = srcs[idx]\n",
    "    idx_content['summary'] = tgts[idx]\n",
    "    wrap.append(idx_content)\n",
    "torch.save(wrap, 'netku_samesecs_train.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3fe501-95dd-4a62-b3cd-4b012ede279d",
   "metadata": {},
   "source": [
    "### Trigger Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "id": "389fd7fc-08dd-4d3a-bdc1-223f7e623309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert/\n",
    "triggers = [clusters[idx][\"wiki_portal_summary\"] for idx in sec_ids]\n",
    "with open(\"samesecs_triggers_test.txt\", \"w\") as f:\n",
    "    for trigger in triggers:\n",
    "        f.write(trigger+\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a3ae5a-b764-48de-9361-889996fe56b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Extract paragraphs from each instance, and label as edited 0/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1103,
   "id": "201d255d-95b9-4811-a539-41ff4e6b398f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled/\n",
    "val_pt = torch.load(\"val_samesecs_labeled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1104,
   "id": "c858ec35-b5fe-4a3c-887b-f6824d67f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the maen modification from each instance, and labeled 1 if bigger than mean value, else 1.\n",
    "# Concate the paragraphs with its respective trigger\n",
    "# Calculate the mean editions for each instance\n",
    "mean_editions = []\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "        editions = []\n",
    "        num_editions = len(re.findall(r\"\\[ADD]\", par)) + len(re.findall(r\"\\[SUB]\", par)) + len(re.findall(r\"\\[RM]\", par))\n",
    "        editions.append(num_editions)\n",
    "    mean_editions.append(np.mean(editions))\n",
    "\n",
    "labels = []\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "        if (len(re.findall(r\"\\[ADD]\", par)) + len(re.findall(r\"\\[SUB]\", par)) + len(re.findall(r\"\\[RM]\", par))) > np.mean(mean_editions[idx]): labels.append(1)\n",
    "        else: labels.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1105,
   "id": "ae05b695-b1bf-4892-85f7-f770c2b56d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# col[0]: paragraph + section name\n",
    "col_0 = []\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "    # for par in val_pt[idx][\"document\"].split(\"\\\\c\\\\c\"):\n",
    "        col_0.append(par)\n",
    "    \n",
    "# col[1]: trigger\n",
    "triggers, col_1 = [], []\n",
    "with open(\"samesecs_triggers_val.txt\", \"r\") as f:\n",
    "    for trigger in f.readlines():\n",
    "        triggers.append(trigger)\n",
    "for idx in range(len(val_pt)):\n",
    "    for par in val_pt[idx][\"summary\"].split(\"\\\\c\\\\c\"):\n",
    "        col_1.append(triggers[idx])\n",
    "\n",
    "# col[2]: target (label)\n",
    "col_2 = labels.copy()\n",
    "assert len(col_0)==len(col_1)==len(col_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1106,
   "id": "68e37ee5-047d-4ba8-aee4-32c47ab74bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the sentences with [ADD] at head\n",
    "old_par = col_0.copy()\n",
    "instances_par = []\n",
    "for idx in range(len(old_par)):\n",
    "    splits = re.split(r\"\\[KEEP\\]\", old_par[idx])\n",
    "    splits = [ele for ele in splits if ele]\n",
    "    for split in splits:\n",
    "        try:\n",
    "            if split.split()[0] in [\"[ADD]\", \"[SUB]\"]: splits.remove(split)\n",
    "        except:\n",
    "            pass\n",
    "    instance = (\"\".join(splits)).replace(\"  \", \" \")\n",
    "    instances_par.append(instance.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1107,
   "id": "9f86b62f-bdcd-4416-8fd6-7092d6491964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert cols into dataframe\n",
    "assert len(instances_par)==len(col_1)==len(col_2)\n",
    "df = pd.DataFrame()\n",
    "df[\"paragraphs_secs\"] = instances_par\n",
    "df[\"trigger\"] = col_1\n",
    "df[\"target\"] = col_2\n",
    "df.to_csv(\"merged_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1108,
   "id": "d65df142-c7a1-4796-beba-3d27df844723",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concate the paragraphs and tirgger columns into sources\n",
    "df = pd.read_csv(\"merged_val.csv\")\n",
    "concats = []\n",
    "for idx in range(df.shape[0]):\n",
    "    row = df.iloc[idx, :]\n",
    "    concated = str(row[\"paragraphs_secs\"]) + \" -- \" + str(row[\"trigger\"])\n",
    "    concats.append(concated)\n",
    "assert len(concats)==df[\"target\"].shape[0]\n",
    "df_updated = pd.DataFrame()\n",
    "df_updated[\"paragraph\"] = concats\n",
    "df_updated[\"target\"] = df[\"target\"].tolist()\n",
    "df_updated.to_csv(\"merged_updated_val.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1110,
   "id": "709e8d75-0e6e-4e05-98d5-bfc5af0625df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paragraph    On October 8, 2020, the U.S. Federal Bureau of...\n",
       "target                                                       0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 1110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"merged_updated_val.csv\")\n",
    "df_train.iloc[0, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ae1652-ce89-4f57-aff3-8ae6c27336d9",
   "metadata": {},
   "source": [
    "### Match paragraphs to respective instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 910,
   "id": "fc54b6cc-55ef-49c6-93dc-f941aa48d55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled\n"
     ]
    }
   ],
   "source": [
    "%cd /Users/quert/Downloads/gcp_tmp/ptfile/same_secs_insert_labeled/\n",
    "test_pt = torch.load(\"test_samesecs_labeled.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 911,
   "id": "b5125a72-edd7-4a58-90ca-8d03a9cd3098",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_par = []\n",
    "for idx in range(len(test_pt)):\n",
    "    num_par.append(len(test_pt[idx][\"summary\"].split(\"\\\\c\\\\c\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "88e5ddad-05d7-42fe-92ff-5b8a82dfc8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame({\"Num of pars\": num_par}).to_csv(\"./merged_numpar_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570bbcb1-1fb6-475d-afec-ff50a8a68eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10425 entries, 0 to 10424\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   paragraph  10425 non-null  object\n",
      " 1   target     10425 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 163.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"merged_updated_test.csv\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "id": "4d9b0bf9-586b-4678-b350-c64051a7a139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        0\n",
       "1        0\n",
       "2        0\n",
       "3        0\n",
       "4        0\n",
       "        ..\n",
       "10420    0\n",
       "10421    0\n",
       "10422    0\n",
       "10423    0\n",
       "10424    0\n",
       "Name: target, Length: 10425, dtype: int64"
      ]
     },
     "execution_count": 920,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_slice = df.iloc[:, 1]\n",
    "df_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "id": "5e082999-62e6-4503-9f2f-865b0e5c5743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10425"
      ]
     },
     "execution_count": 924,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list = df_slice.to_list()\n",
    "len(df_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e51f6f-df9f-4c3b-ba43-78d524fd606c",
   "metadata": {},
   "source": [
    "#### Match paragraphs to its respective instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "id": "f0e93adf-c723-468f-8313-47bc28d237a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"submission.csv\")\n",
    "test_csv = pd.read_csv(\"merged_updated_test.csv\")\n",
    "num_par = pd.read_csv(\"merged_numpar_test.csv\")\n",
    "par_nums = num_par.iloc[:, 1].to_list()\n",
    "assert len(sub) == np.sum(num_par.iloc[:, 1].to_list()) == len(test_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1121,
   "id": "221d2399-e505-4359-9ddd-2ec1bb3d2119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10425"
      ]
     },
     "execution_count": 1121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge paragraphs to instances\n",
    "instances = []\n",
    "start = 0\n",
    "# for num in par_nums:\n",
    "for idx in range(len(par_nums)):\n",
    "    end_idx = start_idx + par_nums[idx] - 1\n",
    "    the_range = [start_idx, end_idx]\n",
    "    start = end_idx + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a7ef9a-aebd-455a-8a10-d51b45e7552f",
   "metadata": {},
   "source": [
    "### Fix the unbalanced problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1111,
   "id": "b6f4c913-1833-4d88-841f-957feb040c48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 73846 entries, 0 to 73845\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   paragraph  73846 non-null  object\n",
      " 1   target     73846 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"merged_updated_train.csv\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1113,
   "id": "8ad46178-5d48-4402-9e90-0cabb0e55bbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1972, 71874)"
      ]
     },
     "execution_count": 1113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df.iloc[:, 1].to_list()\n",
    "true_idx = [i for i in range(len(target)) if target[i]==1]\n",
    "false_idx = [i for i in range(len(target)) if target[i]==0]\n",
    "len(true_idx), len(false_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1114,
   "id": "a70f826f-8d3f-43c9-bb80-aa1c09602569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we sample 3 times of false instances\n",
    "from random import sample\n",
    "resample_false_idx = sorted(sample(false_idx, 2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1115,
   "id": "5408a84f-d937-480a-ac76-8732ac7638f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the false example\n",
    "resample_par, resample_idx = [], []\n",
    "for idx in resample_false_idx:\n",
    "    resample_par.append(df.iloc[idx, 0])\n",
    "    resample_idx.append(df.iloc[idx, 1])\n",
    "# combline the false and true example\n",
    "for idx in true_idx:\n",
    "    resample_par.append(df.iloc[idx, 0])\n",
    "    resample_idx.append(df.iloc[idx, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1116,
   "id": "41c50226-de6f-4291-b8bd-d7b35189b45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({\"paragraph\": resample_par, \"target\": resample_idx}).to_csv(\"resample_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a83a64f-4288-4b92-ad31-ee879ab35f24",
   "metadata": {},
   "source": [
    "### Examine the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "e5e8dd8a-7e11-4222-b915-d2c09bb58b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10425 entries, 0 to 10424\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   target  10425 non-null  int64\n",
      "dtypes: int64(1)\n",
      "memory usage: 81.6 KB\n"
     ]
    }
   ],
   "source": [
    "df_submission = pd.read_csv(\"submission.csv\")\n",
    "df_submission.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 974,
   "id": "071dc1cf-2202-4b3c-b928-8318c429a4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10421\n",
       "1        4\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 974,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "id": "8026fcbc-00fa-4522-92c9-0c83fc502f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10425 entries, 0 to 10424\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   paragraph  10425 non-null  object\n",
      " 1   target     10425 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 163.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# check the test data targets\n",
    "df_test = pd.read_csv(\"merged_updated_test.csv\")\n",
    "df_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "6d2ec77c-d644-40eb-a280-d3957ae150e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    10104\n",
       "1      321\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 971,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.target.value_counts()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
