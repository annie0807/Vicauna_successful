{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e35da190-e9b0-4570-ad83-e0da1084993c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import difflib\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d13396d-c84e-43a9-b01f-42a23a556ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_package = 'en_core_web_sm'\n",
    "nlp_ner = None\n",
    "\n",
    "def get_nlp():\n",
    "    nlp = None\n",
    "    if not nlp:\n",
    "        try:\n",
    "            nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "        except:\n",
    "            import subprocess\n",
    "            print('downloading spacy...')\n",
    "            subprocess.run(\"python3 -m spacy download %s\" % spacy_package, shell=True)\n",
    "            nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "    return nlp\n",
    "\n",
    "def get_nlp_ner():\n",
    "    global nlp_ner\n",
    "    if not nlp_ner:\n",
    "        nlp_ner = spacy.load(spacy_package, disable=[\"tagger\"])  # just the parser\n",
    "    return nlp_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7523bebf-37b2-42a1-af0a-b4b732fe4dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_filter = [\n",
    "    'Share on WhatsApp',\n",
    "    'Share on Messenger',\n",
    "    'Reuse this content',\n",
    "    'Share on LinkedIn',\n",
    "    'Share on Pinterest' ,\n",
    "    'Share on Google+',\n",
    "    'Listen /',\n",
    "    '– Politics Weekly',\n",
    "    'Sorry your browser does not support audio',\n",
    "    'https://flex.acast.com',\n",
    "    '|',\n",
    "    'Share on Facebook',\n",
    "    'Share on Twitter',\n",
    "    'Share via Email',\n",
    "    'Sign up to receive',\n",
    "    'This article is part of a series',\n",
    "    'Follow Guardian',\n",
    "    'Twitter, Facebook and Instagram',\n",
    "    'UK news news',\n",
    "    'Click here to upload it',\n",
    "    'Do you have a photo',\n",
    "    'Listen /',\n",
    "    'Email View',\n",
    "    'Read more Guardian',\n",
    "    'This series is',\n",
    "    'Readers can recommend ',\n",
    "    'UK news news',\n",
    "    'Join the debate',\n",
    "    'guardian.letters@theguardian.com',\n",
    "    'More information',\n",
    "    'Close',\n",
    "    'All our journalism is independent',\n",
    "    'is delivered to thousands of inboxes every weekday',\n",
    "    'with today’s essential stories',\n",
    "    'Newsflash:',\n",
    "    'You can read terms of service here',\n",
    "    'Guardian rating:',\n",
    "    'By clicking on an affiliate link',\n",
    "    'morning briefing news',\n",
    "    'Analysis:',\n",
    "    'Good morning, and welcome to our rolling coverage',\n",
    "    'South and Central Asia news',\n",
    "    'f you have a direct question',\n",
    "    'sign up to the',\n",
    "    'You can read terms of service here.',\n",
    "    'If you want to attract my attention quickly, it is probably better to use Twitter.',\n",
    "    'UK news',\n",
    "]\n",
    "to_filter = list(map(lambda x: x.lower(), to_filter))\n",
    "starts_with = [\n",
    "    'Updated ',\n",
    "    'Here’s the sign-up',\n",
    "    '[Read more on',\n",
    "    '[Here’s the list of',\n",
    "    '[Follow our live coverage',\n",
    "    '[',\n",
    "]\n",
    "contains = [\n",
    "    'Want to get this briefing by email',\n",
    "    'Thank youTo'\n",
    "]\n",
    "ends_with = [\n",
    "    ']',\n",
    "]\n",
    "last_line_re = re.compile('Currently monitoring (\\d|\\,)+ news articles')\n",
    "version_re = re.compile('Version \\d+ of \\d+')\n",
    "#---\n",
    "## general res\n",
    "clean_escaped_html = re.compile('&lt;.*?&gt;')\n",
    "end_comma = re.compile(',$')\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "             \"they\",\n",
    "             \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "             \"am\",\n",
    "             \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
    "             \"doing\",\n",
    "             \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "             \"with\",\n",
    "             \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
    "             \"from\",\n",
    "             \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "             \"there\",\n",
    "             \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "             \"such\",\n",
    "             \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\",\n",
    "             \"now\"]\n",
    "stopwords_lemmas = list(set(map(lambda x: x.lemma_, get_nlp()(' '.join(stopwords)))))\n",
    "## lambdas\n",
    "filter_sents = lambda x: not (\n",
    "    any(map(lambda y: y in x, contains)) or\n",
    "    any(map(lambda y: x.startswith(y), starts_with)) or\n",
    "    any(map(lambda y: x.endswith(y), ends_with))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98893a87-d6d3-4220-9297-818386ac540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(s, split_method='spacy'):\n",
    "    if split_method == 'spacy':\n",
    "        return list(map(lambda x: x.text, get_nlp()(s)))\n",
    "    else:\n",
    "        return s.split()\n",
    "\n",
    "get_lemmas = lambda s: list(map(lambda x: x.lemma_.lower(), get_nlp()(s)))\n",
    "filter_stopword_lemmas = lambda word_list: list(filter(lambda x: x not in stopwords_lemmas, word_list))\n",
    "filter_punct = lambda word_list: list(filter(lambda x: x not in string.punctuation, word_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6119c548-0de0-4a93-93fd-0fd25b30d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert string into pars -> do filtering than convert back to string format\n",
    "def filter_lines(a):\n",
    "    if isinstance(a, list):\n",
    "        pars = a\n",
    "    else:\n",
    "        # pars = a.split('</p>')\n",
    "        pars = a.split('\\n\\n')\n",
    "    output = []\n",
    "    for p in pars:\n",
    "        if not any(map(lambda x: x in p.lower(), to_filter)):\n",
    "            output.append(p)\n",
    "    if isinstance(a, list):\n",
    "        return output\n",
    "    else:\n",
    "        return '\\n\\n'.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82958af-f0cb-46c6-b374-9feee5435bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dateline(x):\n",
    "    ## is short enough\n",
    "    length = len(x.split()) < 6\n",
    "    # has a country name\n",
    "    # 1. Does it have an uppercase word?\n",
    "    has_gpe = any(map(lambda x: x.isupper(), x.split()))\n",
    "    # 2. Is there a country name?\n",
    "    if not has_gpe:\n",
    "        for word in get_words(x):\n",
    "            try:\n",
    "                pycountry.countries.search_fuzzy(word)\n",
    "                has_gpe = True\n",
    "                break\n",
    "            except LookupError:\n",
    "                has_gpe = False\n",
    "    # 3. Is there a GPE?\n",
    "    if not has_gpe:\n",
    "        doc = get_nlp_ner()(x)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                has_gpe = True\n",
    "    ##\n",
    "    if length and has_gpe:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e3ecdb0b-0614-4a0c-ac92-d9a0d6ab5d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences\n",
    "def split_sents(a, perform_filter=True):\n",
    "    nlp = get_nlp()\n",
    "    output_sents = []\n",
    "\n",
    "    # deal with dateline (this can really mess things up...)\n",
    "    dateline_dashes = ['—', '–']\n",
    "    for d in dateline_dashes:\n",
    "        dateline = a.split(d)[0]\n",
    "        if is_dateline(dateline): ## find the dateline\n",
    "            ## dateline.\n",
    "            output_sents.append(dateline.strip())\n",
    "            ## all other sentences.\n",
    "            a = d.join(a.split(d)[1:]).strip()\n",
    "            break\n",
    "\n",
    "    # get sentences from each paragraph\n",
    "    # pars = a.split('.\\n\\n')\n",
    "    # get pars, then read the sentences from each par\n",
    "    pars = a.split('.\\n\\n')\n",
    "    for p in pars:\n",
    "        doc = nlp(p)\n",
    "        sents = list(map(lambda x: x.text, doc.sents))\n",
    "        output_sents += sents\n",
    "\n",
    "    \n",
    "    # filter out garbage/repetitive sentences\n",
    "    if perform_filter:\n",
    "        output_sents = filter_lines(output_sents)\n",
    "\n",
    "    # last-minute processing\n",
    "    output_sents = list(map(lambda x: x.strip(), output_sents))\n",
    "\n",
    "    # merge dateline in with the first sentence\n",
    "    if len(output_sents) > 0:\n",
    "        if is_dateline(output_sents[0]):\n",
    "            output_sents = ['—'.join(output_sents[:2])] + output_sents[2:]\n",
    "    # output_sents = '.\\n\\n'.join(output_sents)\n",
    "    return output_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa2de35d-fb64-4ca3-8adb-ddd54dee7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_diff_ratio(s_old, s_new):\n",
    "    s_old_words, s_new_words = get_words(s_old), get_words(s_new)\n",
    "    return difflib.SequenceMatcher(None, s_old_words, s_new_words).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98b7f6dd-2e95-426f-8d74-b4abd41b2fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_diff(l_old, l_new):\n",
    "    vars_old = []\n",
    "    vars_new = []\n",
    "    diffs = list(difflib.ndiff(l_old, l_new))\n",
    "    in_question = False\n",
    "    for idx, item in enumerate(diffs):\n",
    "        label, text = item[0], item[2:]\n",
    "        if label == '?':\n",
    "            continue\n",
    "\n",
    "        elif label == '-':\n",
    "            vars_old.append({\n",
    "                'text': text,\n",
    "                'tag': '-'\n",
    "            })\n",
    "            if (\n",
    "                    ## if something is removed from the old sentnece, a '?' will be present in the next idx\n",
    "                    ((idx < len(diffs) - 1) and (diffs[idx + 1][0] == '?'))\n",
    "                    ## if NOTHING is removed from the old sentence, a '?' might still be present in 2 idxs, unless the next sentence is a - as well.\n",
    "                 or ((idx < len(diffs) - 2) and (diffs[idx + 2][0] == '?') and diffs[idx + 1][0] != '-')\n",
    "            ):\n",
    "                in_question = True\n",
    "                continue\n",
    "\n",
    "            ## test if the sentences are substantially similar, but for some reason ndiff marked them as different.\n",
    "            if (idx < len(diffs) - 1) and (diffs[idx + 1][0] == '+'):\n",
    "                _, text_new = diffs[idx + 1][0], diffs[idx + 1][2:]\n",
    "                if get_word_diff_ratio(text, text_new) > .9:\n",
    "                    in_question = True\n",
    "                    continue\n",
    "\n",
    "            vars_new.append({\n",
    "                'text': '',\n",
    "                'tag': ''\n",
    "            })\n",
    "\n",
    "\n",
    "        elif label == '+':\n",
    "            old_text, new_text = diffs[idx-2][2:], diffs[idx][2:]\n",
    "            sents_ratio = get_word_diff_ratio(old_text, new_text) \n",
    "            \n",
    "            if sents_ratio >= .8:\n",
    "                vars_new.append({\n",
    "                    'text': new_text,\n",
    "                    'tag': ' '\n",
    "                })\n",
    "            elif sents_ratio < .3:\n",
    "                vars_new.append({\n",
    "                    'text': new_text,\n",
    "                    'tag': '+'\n",
    "                })\n",
    "            else:\n",
    "                vars_new.append({\n",
    "                    'text': new_text,\n",
    "                    'tag': '*'\n",
    "                })\n",
    "            # if in_question:\n",
    "            #     in_question = False\n",
    "            # else:\n",
    "            #     vars_old.append({\n",
    "            #         'text':'',\n",
    "            #         'tag': ' '\n",
    "            #     })\n",
    "        else:\n",
    "            vars_old.append({\n",
    "                'text': text,\n",
    "                'tag': ' '\n",
    "            })\n",
    "            vars_new.append({\n",
    "                'text': text,\n",
    "                'tag': ' '\n",
    "            })\n",
    "\n",
    "    return vars_old, vars_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "50f08906-3cdf-4010-a566-c5e7dcff6814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_edits(vo, vn):\n",
    "    clustered_edits = []\n",
    "    current_cluster = []\n",
    "    for o, n in list(zip(vo, vn)):\n",
    "        if (o['tag'] in ['+', '-']) or (n['tag'] in ['+', '-']):\n",
    "            current_cluster.append((o, n))\n",
    "        ##\n",
    "        if o['tag'] == ' ' and n['tag'] == ' ':\n",
    "            if len(current_cluster) > 0:\n",
    "                clustered_edits.append(current_cluster)\n",
    "                current_cluster = []\n",
    "            clustered_edits.append([(o, n)])\n",
    "    if len(current_cluster) > 0:\n",
    "        clustered_edits.append(current_cluster)\n",
    "    return clustered_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960d5f79-4123-4d05-af94-5604bc0373f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents(idx_i, idx_j, a, c):\n",
    "    \"\"\"Merges two sentences without spacing errors.\"\"\"\n",
    "    si_text = c[idx_i][a]['text']\n",
    "    sj_text = c[idx_j][a]['text']\n",
    "\n",
    "    if isinstance(si_text, (list, tuple)):\n",
    "        output_list = list(si_text)\n",
    "    else:\n",
    "        output_list = [(idx_i, si_text)]\n",
    "    if isinstance(sj_text, (list, tuple)):\n",
    "        output_list += sj_text\n",
    "    else:\n",
    "        output_list.append((idx_j, sj_text))\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0919ca97-7cc2-47e6-bed4-1185c7fd701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents_list(t):\n",
    "    t = sorted(t, key=lambda x: x[0])\n",
    "    t = list(map(lambda x: x[1].strip(), t))\n",
    "    t = ' '.join(t)\n",
    "    return ' '.join(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b90127fd-c8aa-4b93-83f7-7beafe997112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_in_interval(c, idx_i, idx_j, version):\n",
    "    idx_small, idx_large = min([idx_i, idx_j]), max([idx_i, idx_j])\n",
    "    return any(map(lambda idx: c[idx][version]['text'].strip() != '',  range(idx_small+1, idx_large)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cb592189-4e51-4226-9455-67f1721c3586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(s, cache):\n",
    "    if isinstance(s, str) and s in cache:\n",
    "        return cache[s], cache\n",
    "    if isinstance(s, list):\n",
    "        s = merge_sents_list(s)\n",
    "    s_lemmas = get_lemmas(s)\n",
    "    s_lemmas = filter_stopword_lemmas(s_lemmas)\n",
    "    s_lemmas = filter_punct(s_lemmas)\n",
    "    cache[s] = s_lemmas\n",
    "    return cache[s], cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2122e0aa-eb6f-4993-b23a-de2d1af1d05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subset(s1_lemmas, s2_lemmas, slack=.5):\n",
    "    \"\"\"Checks if the second sentence is nearly a subset of the first, with up to `slack` words different.\"\"\"\n",
    "    ### get all text (might be a list).\n",
    "    if len(s2_lemmas) > len(s1_lemmas):\n",
    "        return False\n",
    "    if len(s2_lemmas) > 50:\n",
    "        return False\n",
    "    ### check match.\n",
    "    matches = sum(map(lambda word: word in s1_lemmas, s2_lemmas))\n",
    "    return matches >= (len(s2_lemmas) * (1 - slack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "93e10e5b-fdb3-4cd2-904a-7db533d3853e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_text_spots(c, old_spot_idx, new_spot_idx, version):\n",
    "    ## swap text\n",
    "    text_old = c[old_spot_idx][version]['text']\n",
    "    text_new = c[new_spot_idx][version]['text']\n",
    "    c[new_spot_idx][version]['text'] = text_old\n",
    "    c[old_spot_idx][version]['text'] = text_new\n",
    "    ## swap tags\n",
    "    tag_new = c[new_spot_idx][version]['tag']\n",
    "    tag_old = c[old_spot_idx][version]['tag']\n",
    "    c[new_spot_idx][version]['tag'] = tag_old\n",
    "    c[old_spot_idx][version]['tag'] = tag_new\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c9d5c02e-77b7-4650-8059-5f76c6d73da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_cluster(c, slack=.5):\n",
    "    c = list(filter(lambda x: x[0]['text'] != '' or x[1]['text'] != '', c))\n",
    "    old_c = copy.deepcopy(c)\n",
    "    r_c = range(len(c))\n",
    "    keep_going = True\n",
    "    loop_idx = 0\n",
    "    cache = {}\n",
    "\n",
    "    while keep_going:\n",
    "        for active_version in [0, 1]:\n",
    "            inactive_version = abs(active_version - 1)\n",
    "            for idx_i, idx_j in itertools.product(r_c, r_c):\n",
    "                # [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "                idx_i, idx_j = (idx_i, idx_j) if active_version == 0 else (idx_j, idx_i)\n",
    "                if (\n",
    "                        (idx_i != idx_j)\n",
    "                        and (c[idx_j][active_version]['text'] != '')\n",
    "                        # and (c[idx_j][inactive_version]['text'] == '')\n",
    "                        and (c[idx_i][inactive_version]['text'] != '')\n",
    "                ):\n",
    "\n",
    "                    # print('active: %s, idx_i: %s, idx_j: %s' % (active_version, idx_i, idx_j))\n",
    "                    s1_lemmas, cache = lemmatize_sentence(c[idx_i][inactive_version]['text'], cache)\n",
    "                    s2_lemmas, cache = lemmatize_sentence(c[idx_j][active_version]['text'], cache)\n",
    "                    if check_subset(s1_lemmas, s2_lemmas, slack=slack):\n",
    "                        # if there's a match, first check:\n",
    "                        combined_text_active = merge_sents(idx_i, idx_j, active_version, c)\n",
    "                        combined_text_inactive = merge_sents(idx_i, idx_j, inactive_version, c)\n",
    "                        c[idx_j][active_version]['text'] = combined_text_active\n",
    "                        c[idx_i][active_version]['text'] = ''\n",
    "                        c[idx_i][inactive_version]['text'] = combined_text_inactive\n",
    "                        c[idx_j][inactive_version]['text'] = ''\n",
    "                        # print('FOUND')\n",
    "                        # print(c)\n",
    "                        # print('active: %s, idx_i: %s, idx_j: %s' % (active_version, idx_i, idx_j))\n",
    "\n",
    "                        #    1. if the two idx's are adjacent, then move the active.\n",
    "                        if abs(idx_i - idx_j) == 1:\n",
    "                            # print('1.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        #    2. if there's both >=1 active AND >=1 inactive in between, don't do anything.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, active_version) and text_in_interval(c, idx_i, idx_j, inactive_version):\n",
    "                            # print('2.')\n",
    "                            pass\n",
    "\n",
    "                        #    3. if there's text in the active version between the two idx's, move the inactive.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, active_version):\n",
    "                            # print('3.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_j, old_spot_idx=idx_i, version=inactive_version)\n",
    "\n",
    "                        #    4. if there's text in the inactive in between the two idx's, move the active.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, inactive_version):\n",
    "                            # print('4.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        #   5. if there's no text inbetween the idx's in either the active or the inactive, move the active.\n",
    "                        elif not (\n",
    "                                text_in_interval(c, idx_i, idx_j, active_version) and\n",
    "                                text_in_interval(c, idx_i, idx_j, inactive_version)\n",
    "                        ):\n",
    "                            # print('5.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        ## merge list/text\n",
    "                        for idx, version in itertools.product([idx_i, idx_j], [active_version, inactive_version]):\n",
    "                            if isinstance(c[idx][version]['text'], list):\n",
    "                                c[idx][version]['text'] = merge_sents_list(c[idx][version]['text'])\n",
    "\n",
    "        ## one more merge for safety\n",
    "        for idx, version in itertools.product(r_c, [active_version, inactive_version]):\n",
    "            if isinstance(c[idx][version]['text'], list):\n",
    "                c[idx][version]['text'] = merge_sents_list(c[idx][version]['text'])\n",
    "\n",
    "        if (c == old_c) or (loop_idx > 10000):\n",
    "            # print('done, idx: %s' % loop_idx)\n",
    "            keep_going = False\n",
    "            loop_idx = 0\n",
    "        else:\n",
    "            loop_idx += 1\n",
    "            # print('one more')\n",
    "            old_c = copy.deepcopy(c)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "773e1a7a-392e-46c5-9158-c3a3450f0c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_clusters(vo, vn, slack=.5):\n",
    "    clustered_edits = cluster_edits(vo, vn)\n",
    "    output_edits = []\n",
    "    for c in clustered_edits:\n",
    "        if len(c) == 1:\n",
    "            c_i = c[0]\n",
    "            if not (c_i[0]['text'] == '' and c_i[1]['text'] == ''):\n",
    "                output_edits.append(c_i)\n",
    "        else:\n",
    "            c_new = merge_cluster(c, slack=slack)\n",
    "            for c_i in c_new:\n",
    "                if not (c_i[0]['text'] == '' and c_i[1]['text'] == ''):\n",
    "                    output_edits.append(c_i)\n",
    "\n",
    "    if len(output_edits) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return zip(*output_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9faacef5-78fe-49bf-9e14-481e6a7964af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_diff(a_old, a_new, filter_common_sents=True, merge_clusters=True, slack=.5):\n",
    "    ## split sentences\n",
    "    a_old_sents = split_sents(a_old)\n",
    "    a_new_sents = split_sents(a_new)\n",
    "    if filter_common_sents:\n",
    "        a_old_sents = list(filter(filter_sents, a_old_sents))\n",
    "        a_new_sents = list(filter(filter_sents, a_new_sents))\n",
    "    ## group list\n",
    "    vers_old, vers_new = get_list_diff(a_old_sents, a_new_sents)\n",
    "    ## fix errors/ align sentences\n",
    "    if merge_clusters:\n",
    "        vers_old, vers_new = merge_all_clusters(vers_old, vers_new, slack=slack)\n",
    "    return vers_old, vers_new "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48ac659e-a102-41c8-86ab-5387284ab596",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt = torch.load('./ptfile/test.pt')\n",
    "# bs3_test_pt = torch.load('./ptfile/bs3/test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "46d34bc2-4ced-44c6-a3d1-9b0a6e178a97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The COVID-19 pandemic in Brunei is part of the worldwide pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The virus spread to Brunei on 9 March 2020, when its first case was confirmed in Tutong. Many early cases were linked to Jamek Mosque Sri Petaling in Kuala Lumpur, which held a large Tablighi Jamaat ijtema event at the end of February 2020. Of Bruneis first 50 cases, 45 were related to Jamek Mosque.  The pandemic had spread to all districts of Brunei, except in the exclave of Temburong.\\n\\nBrunei reports a record 42 new cases of COVID-19 in the past 24 hours after reporting their first local infections in 15 months, thereby bringing the nationwide total of confirmed cases to 406.'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old_ver = test_pt[6]['document']\n",
    "old_ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "67a8a3be-60de-4a50-8be7-467dc8dab9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The COVID-19 pandemic in Brunei is part of the worldwide pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The virus spread to Brunei on 9 March 2020, when its first case was confirmed in Tutong. Many early cases were linked to Jamek Mosque Sri Petaling in Kuala Lumpur, which held a large Tablighi Jamaat ijtema event at the end of February 2020. Of Bruneis first 50 cases, 45 were related to Jamek Mosque.  The pandemic had spread to all districts of Brunei, except in the exclave of Temburong.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_ver = test_pt[6]['summary']\n",
    "new_ver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c46c70ea-49c1-413e-801b-8be6dd928927",
   "metadata": {},
   "outputs": [],
   "source": [
    "old, new = get_sentence_diff(old_ver, new_ver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d54aced5-239e-44cc-bdab-96c1bcaa5910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'The COVID-19 pandemic in Brunei is part of the worldwide pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).',\n",
       "  'tag': ' '},\n",
       " {'text': 'The virus spread to Brunei on 9 March 2020, when its first case was confirmed in Tutong.',\n",
       "  'tag': ' '},\n",
       " {'text': 'Many early cases were linked to Jamek Mosque Sri Petaling in Kuala Lumpur, which held a large Tablighi Jamaat ijtema event at the end of February 2020.',\n",
       "  'tag': ' '},\n",
       " {'text': 'Of Bruneis first 50 cases, 45 were related to Jamek Mosque.',\n",
       "  'tag': ' '},\n",
       " {'text': 'The pandemic had spread to all districts of Brunei, except in the exclave of Temburong',\n",
       "  'tag': '-'},\n",
       " {'text': 'Brunei reports a record 42 new cases of COVID-19 in the past 24 hours after reporting their first local infections in 15 months, thereby bringing the nationwide total of confirmed cases to 406.',\n",
       "  'tag': '-'})"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "faa3f8c7-8486-4bc7-906d-a65adfb5b7de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'The COVID-19 pandemic in Brunei is part of the worldwide pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2).',\n",
       "  'tag': ' '},\n",
       " {'text': 'The virus spread to Brunei on 9 March 2020, when its first case was confirmed in Tutong.',\n",
       "  'tag': ' '},\n",
       " {'text': 'Many early cases were linked to Jamek Mosque Sri Petaling in Kuala Lumpur, which held a large Tablighi Jamaat ijtema event at the end of February 2020.',\n",
       "  'tag': ' '},\n",
       " {'text': 'Of Bruneis first 50 cases, 45 were related to Jamek Mosque.',\n",
       "  'tag': ' '},\n",
       " {'text': 'The pandemic had spread to all districts of Brunei, except in the exclave of Temburong.',\n",
       "  'tag': '+'},\n",
       " {'text': '', 'tag': ''})"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "073d0f9d-e54f-4325-9543-51d5f4677cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# old: [RM], [KEEP]\n",
    "# new: [SUB], [ADD]\n",
    "def Bidirectional_merge(old, new):\n",
    "    labeled_data = []\n",
    "    # len(old)==len(new)\n",
    "    for idx in range(len(new)):\n",
    "        if old[idx]['tag']==' ' and new[idx]['tag']==' ':\n",
    "            # labeled_data.append(' [KEEP] ' + new[idx]['text'] + ' [/KEEP]')\n",
    "            labeled_data.append(' [KEEP] ' + old[idx]['text'])\n",
    "        elif old[idx]['tag']=='-' and new[idx]['tag']=='*':\n",
    "            labeled_data.append(' [RM] '+old[idx]['text'])\n",
    "            labeled_data.append(' [SUB] '+new[idx]['text'])\n",
    "        elif old[idx]['tag']=='-' and new[idx]['tag']=='+':\n",
    "            labeled_data.append(' [RM] '+old[idx]['text'])\n",
    "            labeled_data.append(' [ADD] '+new[idx]['text'])\n",
    "        elif old[idx]['tag']=='-' and new[idx]['tag']=='':\n",
    "            labeled_data.append(' [RM] '+old[idx]['text'])\n",
    "        elif new[idx]['tag']=='+' and new[idx]['text']!='': \n",
    "            # labeled_data.append(' [ADD] '+new[idx]['text']+' [/ADD]')\n",
    "            labeled_data.append(' [ADD] '+new[idx]['text'])\n",
    "        # elif new[idx]['tag']=='*' and new[idx]['text']!='':\n",
    "        # else:\n",
    "        #     # labeled_data.append(' [SUB] '+new[idx]['text']+' [/SUB]')\n",
    "        #     labeled_data.append(' [SUB] '+new[idx]['text'])\n",
    "    return ''.join(labeled_data).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3f2888c6-ff9d-4d10-908e-c30d0191192c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[KEEP] The COVID-19 pandemic in Brunei is part of the worldwide pandemic of coronavirus disease 2019 (COVID-19) caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). [KEEP] The virus spread to Brunei on 9 March 2020, when its first case was confirmed in Tutong. [KEEP] Many early cases were linked to Jamek Mosque Sri Petaling in Kuala Lumpur, which held a large Tablighi Jamaat ijtema event at the end of February 2020. [KEEP] Of Bruneis first 50 cases, 45 were related to Jamek Mosque. [RM] The pandemic had spread to all districts of Brunei, except in the exclave of Temburong [ADD] The pandemic had spread to all districts of Brunei, except in the exclave of Temburong. [RM] Brunei reports a record 42 new cases of COVID-19 in the past 24 hours after reporting their first local infections in 15 months, thereby bringing the nationwide total of confirmed cases to 406.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = Bidirectional_merge(old, new)\n",
    "merged"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
