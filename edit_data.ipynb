{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c88c443d-8767-480e-8c70-cbe4af3451d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b155ff00-31e9-4372-84b6-812059fba72a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/Quert/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42c492aa-18f4-4ff4-a041-1d900f4ef324",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dd73f53f-c70e-498e-808b-5de93a4209aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/NetKu/first_paragraph\n"
     ]
    }
   ],
   "source": [
    "%cd ./NetKu/first_paragraph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d087c63e-8667-4b79-9f9c-ba9c04094691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_pt = torch.load('test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10373847-985a-4246-ba52-b7a75ca5ee73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "import difflib\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import pycountry\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000cc82a-0047-41f2-9e6b-0587222208da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nlp():\n",
    "    nlp = None\n",
    "    spacy_package = 'en_core_web_sm'\n",
    "    if not nlp:\n",
    "        try:\n",
    "            nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "        except:\n",
    "            import subprocess\n",
    "            print('downloading spacy...')\n",
    "            subprocess.run(\"python3 -m spacy download %s\" % spacy_package, shell=True)\n",
    "            nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8ffc1ff0-dedd-4a45-be4d-8abdda7e6bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_filter = [\n",
    "    'Share on WhatsApp',\n",
    "    'Share on Messenger',\n",
    "    'Reuse this content',\n",
    "    'Share on LinkedIn',\n",
    "    'Share on Pinterest' ,\n",
    "    'Share on Google+',\n",
    "    'Listen /',\n",
    "    '– Politics Weekly',\n",
    "    'Sorry your browser does not support audio',\n",
    "    'https://flex.acast.com',\n",
    "    '|',\n",
    "    'Share on Facebook',\n",
    "    'Share on Twitter',\n",
    "    'Share via Email',\n",
    "    'Sign up to receive',\n",
    "    'This article is part of a series',\n",
    "    'Follow Guardian',\n",
    "    'Twitter, Facebook and Instagram',\n",
    "    'UK news news',\n",
    "    'Click here to upload it',\n",
    "    'Do you have a photo',\n",
    "    'Listen /',\n",
    "    'Email View',\n",
    "    'Read more Guardian',\n",
    "    'This series is',\n",
    "    'Readers can recommend ',\n",
    "    'UK news news',\n",
    "    'Join the debate',\n",
    "    'guardian.letters@theguardian.com',\n",
    "    'More information',\n",
    "    'Close',\n",
    "    'All our journalism is independent',\n",
    "    'is delivered to thousands of inboxes every weekday',\n",
    "    'with today’s essential stories',\n",
    "    'Newsflash:',\n",
    "    'You can read terms of service here',\n",
    "    'Guardian rating:',\n",
    "    'By clicking on an affiliate link',\n",
    "    'morning briefing news',\n",
    "    'Analysis:',\n",
    "    'Good morning, and welcome to our rolling coverage',\n",
    "    'South and Central Asia news',\n",
    "    'f you have a direct question',\n",
    "    'sign up to the',\n",
    "    'You can read terms of service here.',\n",
    "    'If you want to attract my attention quickly, it is probably better to use Twitter.',\n",
    "    'UK news',\n",
    "]\n",
    "to_filter = list(map(lambda x: x.lower(), to_filter))\n",
    "starts_with = [\n",
    "    'Updated ',\n",
    "    'Here’s the sign-up',\n",
    "    '[Read more on',\n",
    "    '[Here’s the list of',\n",
    "    '[Follow our live coverage',\n",
    "    '[',\n",
    "]\n",
    "contains = [\n",
    "    'Want to get this briefing by email',\n",
    "    'Thank youTo'\n",
    "]\n",
    "ends_with = [\n",
    "    ']',\n",
    "]\n",
    "last_line_re = re.compile('Currently monitoring (\\d|\\,)+ news articles')\n",
    "version_re = re.compile('Version \\d+ of \\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0da8803-06cc-48f0-9360-06ce9a95e501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## general res\n",
    "clean_escaped_html = re.compile('&lt;.*?&gt;')\n",
    "end_comma = re.compile(',$')\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "             \"they\",\n",
    "             \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "             \"am\",\n",
    "             \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
    "             \"doing\",\n",
    "             \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "             \"with\",\n",
    "             \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
    "             \"from\",\n",
    "             \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "             \"there\",\n",
    "             \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "             \"such\",\n",
    "             \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\",\n",
    "             \"now\"]\n",
    "stopwords_lemmas = list(set(map(lambda x: x.lemma_, get_nlp()(' '.join(stopwords)))))\n",
    "## lambdas\n",
    "filter_sents = lambda x: not (\n",
    "    any(map(lambda y: y in x, contains)) or\n",
    "    any(map(lambda y: x.startswith(y), starts_with)) or\n",
    "    any(map(lambda y: x.endswith(y), ends_with))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1499ca31-4fb5-4089-a318-18a5bf7ed7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words(s, split_method='spacy'):\n",
    "    if split_method == 'spacy':\n",
    "        return list(map(lambda x: x.text, get_nlp()(s)))\n",
    "    else:\n",
    "        return s.split()\n",
    "\n",
    "get_lemmas = lambda s: list(map(lambda x: x.lemma_.lower(), get_nlp()(s)))\n",
    "filter_stopword_lemmas = lambda word_list: list(filter(lambda x: x not in stopwords_lemmas, word_list))\n",
    "filter_punct = lambda word_list: list(filter(lambda x: x not in string.punctuation, word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6505a490-de33-4c1c-846f-bd09ed2b36dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_lines(a):\n",
    "    if isinstance(a, list):\n",
    "        pars = a\n",
    "    else:\n",
    "        # pars = a.split('</p>')\n",
    "        pars = a.split('\\n\\n')\n",
    "    output = []\n",
    "    for p in pars:\n",
    "        if not any(map(lambda x: x in p.lower(), to_filter)):\n",
    "            output.append(p)\n",
    "    if isinstance(a, list):\n",
    "        return output\n",
    "    else:\n",
    "        return '\\n\\n'.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4938cc2b-c7fc-4a77-a511-89cf7cae3812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_dateline(x):\n",
    "    ## is short enough\n",
    "    length = len(x.split()) < 6\n",
    "    # has a country name\n",
    "    # 1. Does it have an uppercase word?\n",
    "    has_gpe = any(map(lambda x: x.isupper(), x.split()))\n",
    "    # 2. Is there a country name?\n",
    "    if not has_gpe:\n",
    "        for word in get_words(x):\n",
    "            try:\n",
    "                pycountry.countries.search_fuzzy(word)\n",
    "                has_gpe = True\n",
    "                break\n",
    "            except LookupError:\n",
    "                has_gpe = False\n",
    "    # 3. Is there a GPE?\n",
    "    if not has_gpe:\n",
    "        doc = get_nlp_ner()(x)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                has_gpe = True\n",
    "    ##\n",
    "    if length and has_gpe:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "54130984-0c40-4c37-962a-a33ba7f42b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into sentences\n",
    "def split_sents(a, perform_filter=True):\n",
    "    nlp = get_nlp()\n",
    "    output_sents = []\n",
    "\n",
    "    # deal with dateline (this can really mess things up...)\n",
    "    dateline_dashes = ['—', '–']\n",
    "    for d in dateline_dashes:\n",
    "        dateline = a.split(d)[0]\n",
    "        if is_dateline(dateline): ## find the dateline\n",
    "            ## dateline.\n",
    "            output_sents.append(dateline.strip())\n",
    "            ## all other sentences.\n",
    "            a = d.join(a.split(d)[1:]).strip()\n",
    "            break\n",
    "\n",
    "    # get sentences for each paragraph\n",
    "    # pars = a.split('</p><p>')\n",
    "    pars = a.split('.\\n\\n')\n",
    "    for p in pars:\n",
    "        doc = nlp(p)\n",
    "        sents = list(map(lambda x: x.text, doc.sents))\n",
    "        output_sents += sents\n",
    "\n",
    "    # filter out garbage/repetitive sentences\n",
    "    if perform_filter:\n",
    "        output_sents = filter_lines(output_sents)\n",
    "\n",
    "    # last-minute processing\n",
    "    output_sents = list(map(lambda x: x.strip(), output_sents))\n",
    "\n",
    "    # merge dateline in with the first sentence\n",
    "    if len(output_sents) > 0:\n",
    "        if is_dateline(output_sents[0]):\n",
    "            output_sents = ['—'.join(output_sents[:2])] + output_sents[2:]\n",
    "    return output_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0661f20a-d523-4d84-9903-8cfa48e58b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Running split_sents to docs of each cluster\n",
    "# splitted_docs = []\n",
    "# for idx in range(len(test_pt)):\n",
    "#      splitted_docs.append(split_sents(test_pt[idx]['document'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b9df033-924e-404e-8461-9b0ebdf173af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "201"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# len(splitted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33bdc1-015a-4dc3-bfa8-cce18b8cf45d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we have the splitted_docs saves the splitted sentences from each cluster in order"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e1247-bc30-47d8-ad50-7640b248578c",
   "metadata": {},
   "source": [
    "### Get the diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "32d097e8-d94b-4029-8b15-0a43718baf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_list_diff(l_old, l_new):\n",
    "    vars_old = []\n",
    "    vars_new = []\n",
    "    diffs = list(difflib.ndiff(l_old, l_new))\n",
    "    in_question = False\n",
    "    for idx, item in enumerate(diffs):\n",
    "        label, text = item[0], item[2:]\n",
    "        if label == '?':\n",
    "            continue\n",
    "\n",
    "        elif label == '-':\n",
    "            vars_old.append({\n",
    "                'text': text,\n",
    "                'tag': '-'\n",
    "            })\n",
    "            if (\n",
    "                    ## if something is removed from the old sentnece, a '?' will be present in the next idx\n",
    "                    ((idx < len(diffs) - 1) and (diffs[idx + 1][0] == '?'))\n",
    "                    ## if NOTHING is removed from the old sentence, a '?' might still be present in 2 idxs, unless the next sentence is a - as well.\n",
    "                 or ((idx < len(diffs) - 2) and (diffs[idx + 2][0] == '?') and diffs[idx + 1][0] != '-')\n",
    "            ):\n",
    "                in_question = True\n",
    "                continue\n",
    "\n",
    "            ## test if the sentences are substantially similar, but for some reason ndiff marked them as different.\n",
    "            if (idx < len(diffs) - 1) and (diffs[idx + 1][0] == '+'):\n",
    "                _, text_new = diffs[idx + 1][0], diffs[idx + 1][2:]\n",
    "                if get_word_diff_ratio(text, text_new) > .8:\n",
    "                    in_question = True\n",
    "                    continue\n",
    "\n",
    "            vars_new.append({\n",
    "                'text': '',\n",
    "                'tag': ' '\n",
    "            })\n",
    "\n",
    "        elif label == '+':\n",
    "            vars_new.append({\n",
    "                'text': text,\n",
    "                'tag': '+'\n",
    "            })\n",
    "            if in_question:\n",
    "                in_question = False\n",
    "            else:\n",
    "                vars_old.append({\n",
    "                    'text':'',\n",
    "                    'tag': ' '\n",
    "                })\n",
    "\n",
    "        else:\n",
    "            vars_old.append({\n",
    "                'text': text,\n",
    "                'tag': ' '\n",
    "            })\n",
    "            vars_new.append({\n",
    "                'text': text,\n",
    "                'tag': ' '\n",
    "            })\n",
    "\n",
    "    return vars_old, vars_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9dacbc59-a8c6-4ba4-b68c-c86593f1657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_edits(vo, vn):\n",
    "    clustered_edits = []\n",
    "    current_cluster = []\n",
    "    for o, n in list(zip(vo, vn)):\n",
    "        if (o['tag'] in ['+', '-']) or (n['tag'] in ['+', '-']):\n",
    "            current_cluster.append((o, n))\n",
    "        ##\n",
    "        if o['tag'] == ' ' and n['tag'] == ' ':\n",
    "            if len(current_cluster) > 0:\n",
    "                clustered_edits.append(current_cluster)\n",
    "                current_cluster = []\n",
    "            clustered_edits.append([(o, n)])\n",
    "    if len(current_cluster) > 0:\n",
    "        clustered_edits.append(current_cluster)\n",
    "    return clustered_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "577a8a11-754b-4b18-b590-ffd6f6b80e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(s, cache):\n",
    "    if isinstance(s, str) and s in cache:\n",
    "        return cache[s], cache\n",
    "    if isinstance(s, list):\n",
    "        s = merge_sents_list(s)\n",
    "    s_lemmas = get_lemmas(s)\n",
    "    s_lemmas = filter_stopword_lemmas(s_lemmas)\n",
    "    s_lemmas = filter_punct(s_lemmas)\n",
    "    cache[s] = s_lemmas\n",
    "    return cache[s], cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a606277f-c1a3-4f6c-940e-6467ccfa843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subset(s1_lemmas, s2_lemmas, slack=.5):\n",
    "    \"\"\"Checks if the second sentence is nearly a subset of the first, with up to `slack` words different.\"\"\"\n",
    "    ### get all text (might be a list).\n",
    "    if len(s2_lemmas) > len(s1_lemmas):\n",
    "        return False\n",
    "    if len(s2_lemmas) > 50:\n",
    "        return False\n",
    "    ### check match.\n",
    "    matches = sum(map(lambda word: word in s1_lemmas, s2_lemmas))\n",
    "    return matches >= (len(s2_lemmas) * (1 - slack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f92ec9bf-6619-4e14-9e33-d8f8a9a71dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def merge_cluster(c, slack=.5):\n",
    "    c = list(filter(lambda x: x[0]['text'] != '' or x[1]['text'] != '', c))\n",
    "    old_c = copy.deepcopy(c)\n",
    "    r_c = range(len(c))\n",
    "    keep_going = True\n",
    "    loop_idx = 0\n",
    "    cache = {}\n",
    "\n",
    "    while keep_going:\n",
    "        for active_version in [0, 1]:\n",
    "            inactive_version = abs(active_version - 1)\n",
    "            for idx_i, idx_j in itertools.product(r_c, r_c):\n",
    "                # [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "                idx_i, idx_j = (idx_i, idx_j) if active_version == 0 else (idx_j, idx_i)\n",
    "                if (\n",
    "                        (idx_i != idx_j)\n",
    "                        and (c[idx_j][active_version]['text'] != '')\n",
    "                        # and (c[idx_j][inactive_version]['text'] == '')\n",
    "                        and (c[idx_i][inactive_version]['text'] != '')\n",
    "                ):\n",
    "\n",
    "                    # print('active: %s, idx_i: %s, idx_j: %s' % (active_version, idx_i, idx_j))\n",
    "                    s1_lemmas, cache = lemmatize_sentence(c[idx_i][inactive_version]['text'], cache)\n",
    "                    s2_lemmas, cache = lemmatize_sentence(c[idx_j][active_version]['text'], cache)\n",
    "                    if check_subset(s1_lemmas, s2_lemmas, slack=slack):\n",
    "                        # if there's a match, first check:\n",
    "                        combined_text_active = merge_sents(idx_i, idx_j, active_version, c)\n",
    "                        combined_text_inactive = merge_sents(idx_i, idx_j, inactive_version, c)\n",
    "                        c[idx_j][active_version]['text'] = combined_text_active\n",
    "                        c[idx_i][active_version]['text'] = ''\n",
    "                        c[idx_i][inactive_version]['text'] = combined_text_inactive\n",
    "                        c[idx_j][inactive_version]['text'] = ''\n",
    "                        # print('FOUND')\n",
    "                        # print(c)\n",
    "                        # print('active: %s, idx_i: %s, idx_j: %s' % (active_version, idx_i, idx_j))\n",
    "\n",
    "                        #    1. if the two idx's are adjacent, then move the active.\n",
    "                        if abs(idx_i - idx_j) == 1:\n",
    "                            # print('1.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        #    2. if there's both >=1 active AND >=1 inactive in between, don't do anything.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, active_version) and text_in_interval(c, idx_i, idx_j, inactive_version):\n",
    "                            # print('2.')\n",
    "                            pass\n",
    "\n",
    "                        #    3. if there's text in the active version between the two idx's, move the inactive.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, active_version):\n",
    "                            # print('3.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_j, old_spot_idx=idx_i, version=inactive_version)\n",
    "\n",
    "                        #    4. if there's text in the inactive in between the two idx's, move the active.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, inactive_version):\n",
    "                            # print('4.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        #   5. if there's no text inbetween the idx's in either the active or the inactive, move the active.\n",
    "                        elif not (\n",
    "                                text_in_interval(c, idx_i, idx_j, active_version) and\n",
    "                                text_in_interval(c, idx_i, idx_j, inactive_version)\n",
    "                        ):\n",
    "                            # print('5.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        ## merge list/text\n",
    "                        for idx, version in itertools.product([idx_i, idx_j], [active_version, inactive_version]):\n",
    "                            if isinstance(c[idx][version]['text'], list):\n",
    "                                c[idx][version]['text'] = merge_sents_list(c[idx][version]['text'])\n",
    "\n",
    "        ## one more merge for safety\n",
    "        for idx, version in itertools.product(r_c, [active_version, inactive_version]):\n",
    "            if isinstance(c[idx][version]['text'], list):\n",
    "                c[idx][version]['text'] = merge_sents_list(c[idx][version]['text'])\n",
    "\n",
    "        if (c == old_c) or (loop_idx > 10000):\n",
    "            # print('done, idx: %s' % loop_idx)\n",
    "            keep_going = False\n",
    "            loop_idx = 0\n",
    "        else:\n",
    "            loop_idx += 1\n",
    "            # print('one more')\n",
    "            old_c = copy.deepcopy(c)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f14bcb6a-b6a4-4b96-9c91-d3e39c384186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_clusters(vo, vn, slack=.5):\n",
    "    clustered_edits = cluster_edits(vo, vn)\n",
    "    output_edits = []\n",
    "    for c in clustered_edits:\n",
    "        if len(c) == 1:\n",
    "            c_i = c[0]\n",
    "            if not (c_i[0]['text'] == '' and c_i[1]['text'] == ''):\n",
    "                output_edits.append(c_i)\n",
    "        else:\n",
    "            c_new = merge_cluster(c, slack=slack)\n",
    "            for c_i in c_new:\n",
    "                if not (c_i[0]['text'] == '' and c_i[1]['text'] == ''):\n",
    "                    output_edits.append(c_i)\n",
    "\n",
    "    if len(output_edits) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return zip(*output_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "682c4095-63a9-4392-aa87-b6e1e9a0e607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sentence_diff(a_old, a_new, filter_common_sents=True, merge_clusters=True, slack=.5):\n",
    "    ## split sentences\n",
    "    a_old_sents = split_sents(a_old)\n",
    "    a_new_sents = split_sents(a_new)\n",
    "    if filter_common_sents:\n",
    "        a_old_sents = list(filter(filter_sents, a_old_sents))\n",
    "        a_new_sents = list(filter(filter_sents, a_new_sents))\n",
    "    ## group list\n",
    "    vers_old, vers_new = get_list_diff(a_old_sents, a_new_sents)\n",
    "    ## fix errors/ align sentences\n",
    "    if merge_clusters:\n",
    "        vers_old, vers_new = merge_all_clusters(vers_old, vers_new, slack=slack)\n",
    "    return vers_old, vers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ae996a9f-57b9-412a-82d6-7ad6c2b0cc8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The 2021 Taliban offensive is an ongoing military offensive by the Taliban and allied militant groups, including al-Qaeda, against the government of Afghanistan and its allies that began on 1 May 2021, simultaneous with the withdrawal of most U.S. troops from Afghanistan. As of 15 July, over a third of Afghanistans 421 districts were controlled by the Taliban, and by 21 July, half of Afghanistan was under Taliban control..\\n\\nThe Taliban captures Zaranj, the provincial capital of Nimruz Province, making it the first major capture of a provincial city by the group since the 2001 invasion. A spokesperson for the local police confirms that the city has fallen into Taliban control and blames the lack of reinforcements of military from the central government.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pt[0]['document'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "326d7833-a97d-41b7-a86d-94ce63b3c381",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The 2021 Taliban offensive is an ongoing military offensive by the Taliban and allied militant groups, including al-Qaeda, against the government of Afghanistan and its allies that began on 1 May 2021, simultaneous with the withdrawal of most U.S. troops from Afghanistan. As of 15 July, over a third of Afghanistan's 421 districts were controlled by the Taliban, and by 21 July, half of Afghanistan was under Taliban control.\""
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pt[0]['summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc179c95-4fa1-480b-bcf8-63b285c33461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for get_sentence_diff input\n",
    "# for idx in range(len(test_pt)):\n",
    "idx = 0\n",
    "a_old_sents_input = test_pt[idx]['document'][0]\n",
    "a_new_sents_input = test_pt[idx]['summary']\n",
    "vers_old, vers_new = get_sentence_diff(a_old_sents_input, a_new_sents_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "280fbee4-f819-480a-be32-0880d7b560d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tuple, tuple)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(vers_old), type(vers_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "54c54337-d04b-489a-9e3e-5db7d396c596",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'The 2021 Taliban offensive is an ongoing military offensive by the Taliban and allied militant groups, including al-Qaeda, against the government of Afghanistan and its allies that began on 1 May 2021, simultaneous with the withdrawal of most U.S. troops from Afghanistan.',\n",
       "  'tag': ' '},\n",
       " {'text': 'As of 15 July, over a third of Afghanistans 421 districts were controlled by the Taliban, and by 21 July, half of Afghanistan was under Taliban control.',\n",
       "  'tag': '-'},\n",
       " {'text': 'The Taliban captures Zaranj, the provincial capital of Nimruz Province, making it the first major capture of a provincial city by the group since the 2001 invasion.',\n",
       "  'tag': '-'},\n",
       " {'text': 'A spokesperson for the local police confirms that the city has fallen into Taliban control and blames the lack of reinforcements of military from the central government.',\n",
       "  'tag': '-'})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vers_old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d9c90ff6-7149-41d0-a743-462aec5ae700",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'text': 'The 2021 Taliban offensive is an ongoing military offensive by the Taliban and allied militant groups, including al-Qaeda, against the government of Afghanistan and its allies that began on 1 May 2021, simultaneous with the withdrawal of most U.S. troops from Afghanistan.',\n",
       "  'tag': ' '},\n",
       " {'text': \"As of 15 July, over a third of Afghanistan's 421 districts were controlled by the Taliban, and by 21 July, half of Afghanistan was under Taliban control.\",\n",
       "  'tag': '+'},\n",
       " {'text': '', 'tag': ' '},\n",
       " {'text': '', 'tag': ' '})"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1554289a-f754-4843-8f15-dba0d1a36e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d25f74f1-b63c-47ef-91e1-346f2982d098",
   "metadata": {},
   "source": [
    "### end of 0928 progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7816275-1f2e-4d85-8c32-75be78484872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_diffs(s_old, s_new, split_method='spacy'):\n",
    "    s_old_words, s_new_words = get_words(s_old, split_method), get_words(s_new, split_method)\n",
    "    return get_list_diff(s_old_words, s_new_words)\n",
    "\n",
    "\n",
    "def get_word_diff_ratio(s_old, s_new):\n",
    "    s_old_words, s_new_words = get_words(s_old), get_words(s_new)\n",
    "    return difflib.SequenceMatcher(None, s_old_words, s_new_words).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d4a4e0b-6db4-4902-8228-e06f1aa63056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changes(old_doc, new_doc):\n",
    "    new_document = []\n",
    "    old_document = []\n",
    "    new_sentences = []\n",
    "    removed_sentences = []\n",
    "\n",
    "    same_sentences = []\n",
    "    changed_sentence_pairs = []\n",
    "\n",
    "    for s_idx, (s_old, s_new) in enumerate(zip(old_doc, new_doc)):\n",
    "        ###\n",
    "        if s_old['text'].strip() != '':\n",
    "            old_document.append(s_old['text'])\n",
    "        if s_new['text'].strip() != '':\n",
    "            new_document.append(s_new['text'])\n",
    "\n",
    "        ###\n",
    "        if s_old['tag'] == '-' and s_new['tag'] == '+':\n",
    "            changed_sentence_pairs.append((s_idx, (s_old['text'], s_new['text'])))\n",
    "\n",
    "        ###\n",
    "        if s_old['tag'] == ' ' and s_new['tag'] == '+':\n",
    "            new_sentences.append(s_new['text'])\n",
    "\n",
    "        ###\n",
    "        if s_new['tag'] == ' ' and s_old['tag'] == '-':\n",
    "            removed_sentences.append(s_old['text'])\n",
    "\n",
    "        if s_new['tag'] == ' ' and s_old['tag'] == ' ':\n",
    "            same_sentences.append(s_old['text'])\n",
    "\n",
    "    return {\n",
    "        'docs': {'old_doc': old_document,\n",
    "                 'new_doc': new_document,\n",
    "                 },\n",
    "        'sentences': {'added_sents': new_sentences,\n",
    "                      'removed_sents': removed_sentences,\n",
    "                      'changed_sent_pairs': changed_sentence_pairs\n",
    "                      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545be0c0-20d6-49f9-ac46-780433b6a92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
