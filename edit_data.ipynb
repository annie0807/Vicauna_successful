{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88c443d-8767-480e-8c70-cbe4af3451d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b155ff00-31e9-4372-84b6-812059fba72a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/Quert/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42c492aa-18f4-4ff4-a041-1d900f4ef324",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd73f53f-c70e-498e-808b-5de93a4209aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/NetKu/first_paragraph\n"
     ]
    }
   ],
   "source": [
    "%cd ./NetKu/first_paragraph/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d087c63e-8667-4b79-9f9c-ba9c04094691",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pt = torch.load('test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10373847-985a-4246-ba52-b7a75ca5ee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import difflib\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import pycountry\n",
    "import pandas as pd\n",
    "spacy_package = 'en_core_web_sm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "000cc82a-0047-41f2-9e6b-0587222208da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nlp():\n",
    "    # global nlp\n",
    "    # if not nlp:\n",
    "    #     try:\n",
    "    #         nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "    #     except:\n",
    "    #         import subprocess\n",
    "    #         print('downloading spacy...')\n",
    "    #         subprocess.run(\"python3 -m spacy download %s\" % spacy_package, shell=True)\n",
    "    #         nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "    nlp = spacy.load(spacy_package, disable=['tagger', 'ner'])\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8ffc1ff0-dedd-4a45-be4d-8abdda7e6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "to_filter = [\n",
    "    'Share on WhatsApp',\n",
    "    'Share on Messenger',\n",
    "    'Reuse this content',\n",
    "    'Share on LinkedIn',\n",
    "    'Share on Pinterest' ,\n",
    "    'Share on Google+',\n",
    "    'Listen /',\n",
    "    '– Politics Weekly',\n",
    "    'Sorry your browser does not support audio',\n",
    "    'https://flex.acast.com',\n",
    "    '|',\n",
    "    'Share on Facebook',\n",
    "    'Share on Twitter',\n",
    "    'Share via Email',\n",
    "    'Sign up to receive',\n",
    "    'This article is part of a series',\n",
    "    'Follow Guardian',\n",
    "    'Twitter, Facebook and Instagram',\n",
    "    'UK news news',\n",
    "    'Click here to upload it',\n",
    "    'Do you have a photo',\n",
    "    'Listen /',\n",
    "    'Email View',\n",
    "    'Read more Guardian',\n",
    "    'This series is',\n",
    "    'Readers can recommend ',\n",
    "    'UK news news',\n",
    "    'Join the debate',\n",
    "    'guardian.letters@theguardian.com',\n",
    "    'More information',\n",
    "    'Close',\n",
    "    'All our journalism is independent',\n",
    "    'is delivered to thousands of inboxes every weekday',\n",
    "    'with today’s essential stories',\n",
    "    'Newsflash:',\n",
    "    'You can read terms of service here',\n",
    "    'Guardian rating:',\n",
    "    'By clicking on an affiliate link',\n",
    "    'morning briefing news',\n",
    "    'Analysis:',\n",
    "    'Good morning, and welcome to our rolling coverage',\n",
    "    'South and Central Asia news',\n",
    "    'f you have a direct question',\n",
    "    'sign up to the',\n",
    "    'You can read terms of service here.',\n",
    "    'If you want to attract my attention quickly, it is probably better to use Twitter.',\n",
    "    'UK news',\n",
    "]\n",
    "to_filter = list(map(lambda x: x.lower(), to_filter))\n",
    "starts_with = [\n",
    "    'Updated ',\n",
    "    'Here’s the sign-up',\n",
    "    '[Read more on',\n",
    "    '[Here’s the list of',\n",
    "    '[Follow our live coverage',\n",
    "    '[',\n",
    "]\n",
    "contains = [\n",
    "    'Want to get this briefing by email',\n",
    "    'Thank youTo'\n",
    "]\n",
    "ends_with = [\n",
    "    ']',\n",
    "]\n",
    "last_line_re = re.compile('Currently monitoring (\\d|\\,)+ news articles')\n",
    "version_re = re.compile('Version \\d+ of \\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0da8803-06cc-48f0-9360-06ce9a95e501",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/venv/lib/python3.8/site-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
      "  warnings.warn(Warnings.W108)\n"
     ]
    }
   ],
   "source": [
    "## general res\n",
    "clean_escaped_html = re.compile('&lt;.*?&gt;')\n",
    "end_comma = re.compile(',$')\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "             \"they\",\n",
    "             \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "             \"am\",\n",
    "             \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
    "             \"doing\",\n",
    "             \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "             \"with\",\n",
    "             \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
    "             \"from\",\n",
    "             \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "             \"there\",\n",
    "             \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "             \"such\",\n",
    "             \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\",\n",
    "             \"now\"]\n",
    "stopwords_lemmas = list(set(map(lambda x: x.lemma_, get_nlp()(' '.join(stopwords)))))\n",
    "## lambdas\n",
    "filter_sents = lambda x: not (\n",
    "    any(map(lambda y: y in x, contains)) or\n",
    "    any(map(lambda y: x.startswith(y), starts_with)) or\n",
    "    any(map(lambda y: x.endswith(y), ends_with))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1499ca31-4fb5-4089-a318-18a5bf7ed7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words(s, split_method='spacy'):\n",
    "    if split_method == 'spacy':\n",
    "        return list(map(lambda x: x.text, get_nlp()(s)))\n",
    "    else:\n",
    "        return s.split()\n",
    "\n",
    "get_lemmas = lambda s: list(map(lambda x: x.lemma_.lower(), get_nlp()(s)))\n",
    "filter_stopword_lemmas = lambda word_list: list(filter(lambda x: x not in stopwords_lemmas, word_list))\n",
    "filter_punct = lambda word_list: list(filter(lambda x: x not in string.punctuation, word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6505a490-de33-4c1c-846f-bd09ed2b36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_lines(a):\n",
    "    if isinstance(a, list):\n",
    "        pars = a\n",
    "    else:\n",
    "        # pars = a.split('</p>')\n",
    "        pars = a.split('\\n\\n')\n",
    "    output = []\n",
    "    for p in pars:\n",
    "        if not any(map(lambda x: x in p.lower(), to_filter)):\n",
    "            output.append(p)\n",
    "    if isinstance(a, list):\n",
    "        return output\n",
    "    else:\n",
    "        return '\\n\\n'.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4938cc2b-c7fc-4a77-a511-89cf7cae3812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_dateline(x):\n",
    "    ## is short enough\n",
    "    length = len(x.split()) < 6\n",
    "    # has a country name\n",
    "    # 1. Does it have an uppercase word?\n",
    "    has_gpe = any(map(lambda x: x.isupper(), x.split()))\n",
    "    # 2. Is there a country name?\n",
    "    if not has_gpe:\n",
    "        for word in get_words(x):\n",
    "            try:\n",
    "                pycountry.countries.search_fuzzy(word)\n",
    "                has_gpe = True\n",
    "                break\n",
    "            except LookupError:\n",
    "                has_gpe = False\n",
    "    # 3. Is there a GPE?\n",
    "    if not has_gpe:\n",
    "        doc = get_nlp_ner()(x)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                has_gpe = True\n",
    "    ##\n",
    "    if length and has_gpe:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54130984-0c40-4c37-962a-a33ba7f42b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into sentences\n",
    "def split_sents(a, perform_filter=True):\n",
    "    nlp = get_nlp()\n",
    "    output_sents = []\n",
    "\n",
    "    # deal with dateline (this can really mess things up...)\n",
    "    dateline_dashes = ['—', '–']\n",
    "    for d in dateline_dashes:\n",
    "        dateline = a.split(d)[0]\n",
    "        if is_dateline(dateline): ## find the dateline\n",
    "            ## dateline.\n",
    "            output_sents.append(dateline.strip())\n",
    "            ## all other sentences.\n",
    "            a = d.join(a.split(d)[1:]).strip()\n",
    "            break\n",
    "\n",
    "    # get sentences for each paragraph\n",
    "    # pars = a.split('</p><p>')\n",
    "    pars = a.split('\\n\\n')\n",
    "    for p in pars:\n",
    "        doc = nlp(p)\n",
    "        sents = list(map(lambda x: x.text, doc.sents))\n",
    "        output_sents += sents\n",
    "\n",
    "    # filter out garbage/repetitive sentences\n",
    "    if perform_filter:\n",
    "        output_sents = filter_lines(output_sents)\n",
    "\n",
    "    # last-minute processing\n",
    "    output_sents = list(map(lambda x: x.strip(), output_sents))\n",
    "\n",
    "    # merge dateline in with the first sentence\n",
    "    if len(output_sents) > 0:\n",
    "        if is_dateline(output_sents[0]):\n",
    "            output_sents = ['—'.join(output_sents[:2])] + output_sents[2:]\n",
    "    return output_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0661f20a-d523-4d84-9903-8cfa48e58b22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m splitted_docs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_pt)):\n\u001b[0;32m----> 4\u001b[0m      splitted_docs\u001b[38;5;241m.\u001b[39mappend(\u001b[43msplit_sents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pt\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdocument\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Input \u001b[0;32mIn [15]\u001b[0m, in \u001b[0;36msplit_sents\u001b[0;34m(a, perform_filter)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_sents\u001b[39m(a, perform_filter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m----> 3\u001b[0m     nlp \u001b[38;5;241m=\u001b[39m \u001b[43mget_nlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     output_sents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# deal with dateline (this can really mess things up...)\u001b[39;00m\n",
      "Input \u001b[0;32mIn [9]\u001b[0m, in \u001b[0;36mget_nlp\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_nlp\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m nlp\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mnlp\u001b[49m:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m             nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(spacy_package, disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "# Running split_sents to docs of each cluster\n",
    "splitted_docs = []\n",
    "for idx in range(len(test_pt)):\n",
    "     splitted_docs.append(split_sents(test_pt[idx]['document'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd33bdc1-015a-4dc3-bfa8-cce18b8cf45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have the splitted_docs saves the splitted sentences from each cluster in order"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
