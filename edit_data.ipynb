{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c88c443d-8767-480e-8c70-cbe4af3451d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!which python3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b155ff00-31e9-4372-84b6-812059fba72a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ethan/Documents/Quert/data/NetKu/summary\n"
     ]
    }
   ],
   "source": [
    "%cd ~/Documents/Quert/data/NetKu/summary/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10373847-985a-4246-ba52-b7a75ca5ee73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import spacy\n",
    "import difflib\n",
    "import itertools\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import pycountry\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d087c63e-8667-4b79-9f9c-ba9c04094691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_pt = torch.load('train.pt')\n",
    "test_pt = torch.load('test.pt')\n",
    "val_pt = torch.load('val.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "000cc82a-0047-41f2-9e6b-0587222208da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_nlp():\n",
    "    nlp = None\n",
    "    spacy_package = 'en_core_web_sm'\n",
    "    if not nlp:\n",
    "        try:\n",
    "            nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "        except:\n",
    "            import subprocess\n",
    "            print('downloading spacy...')\n",
    "            subprocess.run(\"python3 -m spacy download %s\" % spacy_package, shell=True)\n",
    "            nlp = spacy.load(spacy_package, disable=[\"tagger\" \"ner\"])\n",
    "    return nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ffc1ff0-dedd-4a45-be4d-8abdda7e6bad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_filter = [\n",
    "    'Share on WhatsApp',\n",
    "    'Share on Messenger',\n",
    "    'Reuse this content',\n",
    "    'Share on LinkedIn',\n",
    "    'Share on Pinterest' ,\n",
    "    'Share on Google+',\n",
    "    'Listen /',\n",
    "    '– Politics Weekly',\n",
    "    'Sorry your browser does not support audio',\n",
    "    'https://flex.acast.com',\n",
    "    '|',\n",
    "    'Share on Facebook',\n",
    "    'Share on Twitter',\n",
    "    'Share via Email',\n",
    "    'Sign up to receive',\n",
    "    'This article is part of a series',\n",
    "    'Follow Guardian',\n",
    "    'Twitter, Facebook and Instagram',\n",
    "    'UK news news',\n",
    "    'Click here to upload it',\n",
    "    'Do you have a photo',\n",
    "    'Listen /',\n",
    "    'Email View',\n",
    "    'Read more Guardian',\n",
    "    'This series is',\n",
    "    'Readers can recommend ',\n",
    "    'UK news news',\n",
    "    'Join the debate',\n",
    "    'guardian.letters@theguardian.com',\n",
    "    'More information',\n",
    "    'Close',\n",
    "    'All our journalism is independent',\n",
    "    'is delivered to thousands of inboxes every weekday',\n",
    "    'with today’s essential stories',\n",
    "    'Newsflash:',\n",
    "    'You can read terms of service here',\n",
    "    'Guardian rating:',\n",
    "    'By clicking on an affiliate link',\n",
    "    'morning briefing news',\n",
    "    'Analysis:',\n",
    "    'Good morning, and welcome to our rolling coverage',\n",
    "    'South and Central Asia news',\n",
    "    'f you have a direct question',\n",
    "    'sign up to the',\n",
    "    'You can read terms of service here.',\n",
    "    'If you want to attract my attention quickly, it is probably better to use Twitter.',\n",
    "    'UK news',\n",
    "]\n",
    "to_filter = list(map(lambda x: x.lower(), to_filter))\n",
    "starts_with = [\n",
    "    'Updated ',\n",
    "    'Here’s the sign-up',\n",
    "    '[Read more on',\n",
    "    '[Here’s the list of',\n",
    "    '[Follow our live coverage',\n",
    "    '[',\n",
    "]\n",
    "contains = [\n",
    "    'Want to get this briefing by email',\n",
    "    'Thank youTo'\n",
    "]\n",
    "ends_with = [\n",
    "    ']',\n",
    "]\n",
    "last_line_re = re.compile('Currently monitoring (\\d|\\,)+ news articles')\n",
    "version_re = re.compile('Version \\d+ of \\d+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0da8803-06cc-48f0-9360-06ce9a95e501",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## general res\n",
    "clean_escaped_html = re.compile('&lt;.*?&gt;')\n",
    "end_comma = re.compile(',$')\n",
    "stopwords = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "             \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\",\n",
    "             \"they\",\n",
    "             \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\",\n",
    "             \"am\",\n",
    "             \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\",\n",
    "             \"doing\",\n",
    "             \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\",\n",
    "             \"with\",\n",
    "             \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\",\n",
    "             \"from\",\n",
    "             \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
    "             \"there\",\n",
    "             \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\",\n",
    "             \"such\",\n",
    "             \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"can\", \"will\", \"just\", \"should\",\n",
    "             \"now\"]\n",
    "stopwords_lemmas = list(set(map(lambda x: x.lemma_, get_nlp()(' '.join(stopwords)))))\n",
    "## lambdas\n",
    "filter_sents = lambda x: not (\n",
    "    any(map(lambda y: y in x, contains)) or\n",
    "    any(map(lambda y: x.startswith(y), starts_with)) or\n",
    "    any(map(lambda y: x.endswith(y), ends_with))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1499ca31-4fb5-4089-a318-18a5bf7ed7ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_words(s, split_method='spacy'):\n",
    "    if split_method == 'spacy':\n",
    "        return list(map(lambda x: x.text, get_nlp()(s)))\n",
    "    else:\n",
    "        return s.split()\n",
    "\n",
    "get_lemmas = lambda s: list(map(lambda x: x.lemma_.lower(), get_nlp()(s)))\n",
    "filter_stopword_lemmas = lambda word_list: list(filter(lambda x: x not in stopwords_lemmas, word_list))\n",
    "filter_punct = lambda word_list: list(filter(lambda x: x not in string.punctuation, word_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6505a490-de33-4c1c-846f-bd09ed2b36dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert string into pars -> do filtering than convert back to string format\n",
    "def filter_lines(a):\n",
    "    if isinstance(a, list):\n",
    "        pars = a\n",
    "    else:\n",
    "        # pars = a.split('</p>')\n",
    "        pars = a.split('\\n\\n')\n",
    "    output = []\n",
    "    for p in pars:\n",
    "        if not any(map(lambda x: x in p.lower(), to_filter)):\n",
    "            output.append(p)\n",
    "    if isinstance(a, list):\n",
    "        return output\n",
    "    else:\n",
    "        return '\\n\\n'.join(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4938cc2b-c7fc-4a77-a511-89cf7cae3812",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_dateline(x):\n",
    "    ## is short enough\n",
    "    length = len(x.split()) < 6\n",
    "    # has a country name\n",
    "    # 1. Does it have an uppercase word?\n",
    "    has_gpe = any(map(lambda x: x.isupper(), x.split()))\n",
    "    # 2. Is there a country name?\n",
    "    if not has_gpe:\n",
    "        for word in get_words(x):\n",
    "            try:\n",
    "                pycountry.countries.search_fuzzy(word)\n",
    "                has_gpe = True\n",
    "                break\n",
    "            except LookupError:\n",
    "                has_gpe = False\n",
    "    # 3. Is there a GPE?\n",
    "    if not has_gpe:\n",
    "        doc = get_nlp_ner()(x)\n",
    "        for ent in doc.ents:\n",
    "            if ent.label_ == 'PERSON':\n",
    "                has_gpe = True\n",
    "    ##\n",
    "    if length and has_gpe:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54130984-0c40-4c37-962a-a33ba7f42b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into sentences\n",
    "def split_sents(a, perform_filter=True):\n",
    "    nlp = get_nlp()\n",
    "    output_sents = []\n",
    "\n",
    "    # deal with dateline (this can really mess things up...)\n",
    "    dateline_dashes = ['—', '–']\n",
    "    for d in dateline_dashes:\n",
    "        dateline = a.split(d)[0]\n",
    "        if is_dateline(dateline): ## find the dateline\n",
    "            ## dateline.\n",
    "            output_sents.append(dateline.strip())\n",
    "            ## all other sentences.\n",
    "            a = d.join(a.split(d)[1:]).strip()\n",
    "            break\n",
    "\n",
    "    # get sentences from each paragraph\n",
    "    # pars = a.split('.\\n\\n')\n",
    "    # get pars, then read the sentences from each par\n",
    "    pars = a.split('.\\n\\n')\n",
    "    for p in pars:\n",
    "        doc = nlp(p)\n",
    "        sents = list(map(lambda x: x.text, doc.sents))\n",
    "        output_sents += sents\n",
    "\n",
    "    \n",
    "    # filter out garbage/repetitive sentences\n",
    "    if perform_filter:\n",
    "        output_sents = filter_lines(output_sents)\n",
    "\n",
    "    # last-minute processing\n",
    "    output_sents = list(map(lambda x: x.strip(), output_sents))\n",
    "\n",
    "    # merge dateline in with the first sentence\n",
    "    if len(output_sents) > 0:\n",
    "        if is_dateline(output_sents[0]):\n",
    "            output_sents = ['—'.join(output_sents[:2])] + output_sents[2:]\n",
    "    # output_sents = '.\\n\\n'.join(output_sents)\n",
    "    return output_sents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0661f20a-d523-4d84-9903-8cfa48e58b22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Running split_sents to docs of each cluster\n",
    "# splitted_docs = []\n",
    "# for idx in range(len(test_pt)):\n",
    "#      splitted_docs.append(split_sents(test_pt[idx]['document'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9df033-924e-404e-8461-9b0ebdf173af",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(splitted_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadd4e62-9479-40f0-bcdc-9e1c67a486c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Now we have the splitted_docs saves the splitted sentences from each cluster in order**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649e1247-bc30-47d8-ad50-7640b248578c",
   "metadata": {},
   "source": [
    "### Get the diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "df141d41-7c4a-48a4-932f-e8ed845afc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['- I got the pencil.\\n', '+ I got thes pencil.\\n', '?          +\\n', '  \\n', '- But that is not mine.', '+ But I think that is belong to Leo.']\n"
     ]
    }
   ],
   "source": [
    "# diff = difflib.ndiff('one\\ntwo\\nthree\\n'.splitlines(keepends=True), # one\\ntwo\\nthree\\n\n",
    "#              'ore\\ntno\\nemu\\n'.splitlines(keepends=True))          # ore\\ntno\\nemu\\n\n",
    "\n",
    "# print(''.join(diff), end=\"\")\n",
    "# print('-----')\n",
    "\n",
    "# diff = list(difflib.ndiff('one\\ntwo\\nthree\\n'.splitlines(keepends=True), # one\\ntwo\\nthree\\n\n",
    "#              'ore\\ntree\\nemu\\n'.splitlines(keepends=True)))          # ore\\ntree\\nemu\\n\n",
    "\n",
    "diff = list(difflib.ndiff('I got the pencil.\\n\\nBut that is not mine.'.splitlines(keepends=True), # one\\ntwo\\nthree\\n\n",
    "             'I got thes pencil.\\n\\nBut I think that is belong to Leo.'.splitlines(keepends=True))) # ore\\ntree\\nemu\\n\n",
    "\n",
    "# print(''.join(diff), end=\"\")\n",
    "\n",
    "print(list(diff))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "2ace1c24-5587-47d1-ba51-29190019c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from difflib import ndiff\n",
    "# def modify(src, tgt): # take src in list format, tgt in str format\n",
    "#     src = '\\n\\n'.join(src) # idx is under waiting\n",
    "#     input_docs = src.replace('..\\n\\n', '.\\n\\n') # deal with 's, s, \\s\n",
    "#     input_docs = input_docs.replace(\"'s\", \"s\")\n",
    "#     input_docs = input_docs.replace(\"\\\\\", \"\")\n",
    "#     input_docs = input_docs.strip()\n",
    "#     input_summ = tgt.replace('\\\\\\\\c', '\\n\\n')\n",
    "#     input_summ = input_summ.replace(\"\\\\\\'s\", \"'s\")\n",
    "#     input_summ = input_summ.replace(\"\\'s\", \"s\")\n",
    "#     input_summ = input_summ.replace(\"\\\\\\'\", \"\")\n",
    "#     input_summ = input_summ.strip()\n",
    "#     return input_docs, input_summ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "58bb453c-1a7b-4260-b8b3-5da50362b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs, summs = [], []\n",
    "# for idx in range(len(val_pt)):\n",
    "#     doc = val_pt[idx]['document']\n",
    "#     summ = val_pt[idx]['summary']\n",
    "#     modified_doc, modified_summ = modify(doc, summ)\n",
    "#     docs.append(modified_doc)\n",
    "#     summs.append(modified_summ)\n",
    "# assert len(docs)==len(summs)\n",
    "# print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "4ded80a3-c609-4fa5-869a-b724161074da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Output the processed .pt\n",
    "# wrap = []\n",
    "# # src_strs = output_str.copy()\n",
    "\n",
    "# for idx in range(len(docs)):\n",
    "#     idx_content = {}\n",
    "#     idx_content['document'] = docs[idx]\n",
    "#     idx_content['summary'] = summs[idx]\n",
    "#     wrap.append(idx_content)\n",
    "# torch.save(wrap, 'processed_val.pt')\n",
    "# pro_pt = torch.load('processed_val.pt')\n",
    "# print(len(pro_pt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8be09947-56dd-4308-ad1c-2b195f223d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_diff_ratio(s_old, s_new):\n",
    "    s_old_words, s_new_words = get_words(s_old), get_words(s_new)\n",
    "    return difflib.SequenceMatcher(None, s_old_words, s_new_words).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32d097e8-d94b-4029-8b15-0a43718baf39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_list_diff(l_old, l_new):\n",
    "    vars_old = []\n",
    "    vars_new = []\n",
    "    diffs = list(difflib.ndiff(l_old, l_new))\n",
    "    in_question = False\n",
    "    for idx, item in enumerate(diffs):\n",
    "        label, text = item[0], item[2:]\n",
    "        if label == '?':\n",
    "            continue\n",
    "        # Add the function to compute matching ratio here: similar as below, as an example\n",
    "        # _, text_new = diffs[idx + 1][0], diffs[idx + 1][2:]\n",
    "        elif label == '-':\n",
    "            vars_old.append({\n",
    "                'text': text,\n",
    "                'tag': '-'\n",
    "            })\n",
    "            if (\n",
    "                    ## if something is removed from the old sentnece, a '?' will be present in the next idx\n",
    "                    ((idx < len(diffs) - 1) and (diffs[idx + 1][0] == '?'))\n",
    "                    ## if NOTHING is removed from the old sentence, a '?' might still be present in 2 idxs, unless the next sentence is a - as well.\n",
    "                 or ((idx < len(diffs) - 2) and (diffs[idx + 2][0] == '?') and diffs[idx + 1][0] != '-')\n",
    "            ):\n",
    "                in_question = True\n",
    "                continue\n",
    "\n",
    "            ## test if the sentences are substantially similar, but for some reason ndiff marked them as different.\n",
    "            if (idx < len(diffs) - 1) and (diffs[idx + 1][0] == '+'):\n",
    "                _, text_new = diffs[idx + 1][0], diffs[idx + 1][2:]\n",
    "                if get_word_diff_ratio(text, text_new) > .9:\n",
    "                    in_question = True\n",
    "                    vars_new.append({\n",
    "                        'text': '',\n",
    "                        'tag': ' '\n",
    "                    })\n",
    "        elif label == '+': # and matching ratio<.5, tag with \"#\"\n",
    "            # if label=='+' and matching ratio is between .5 and .9, tag with '#'.\n",
    "            # aka [SUB]\n",
    "            text_old, text_new = diffs[idx-2][2:], diffs[idx][2:] # possible problems here \n",
    "            ratio = get_word_diff_ratio(text_old, text_new) \n",
    "            if ratio > .8:\n",
    "                # vars_old.append({\n",
    "                #     'text':'',\n",
    "                #     'tag': ' '\n",
    "                #     })\n",
    "                vars_new.append({\n",
    "                    'text': text,\n",
    "                    'tag': ' ' # keep\n",
    "                })\n",
    "            elif ratio < .3:\n",
    "                # vars_old.append({\n",
    "                #     'text':'',\n",
    "                #     'tag': ' '\n",
    "                #     })\n",
    "                vars_new.append({\n",
    "                    'text': text,\n",
    "                    'tag': '+' # add\n",
    "                    })\n",
    "            else:\n",
    "                # vars_old.append({\n",
    "                #     'text':'',\n",
    "                #     'tag': ' '\n",
    "                #     })\n",
    "                vars_new.append({\n",
    "                    'text': text,\n",
    "                    'tag': '#' # sub\n",
    "                    })\n",
    "    return vars_old, vars_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9dacbc59-a8c6-4ba4-b68c-c86593f1657a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_edits(vo, vn):\n",
    "    clustered_edits = []\n",
    "    current_cluster = []\n",
    "    for o, n in list(zip(vo, vn)):\n",
    "        if (o['tag'] in ['+', '-']) or (n['tag'] in ['+', '-']):\n",
    "            current_cluster.append((o, n))\n",
    "        ##\n",
    "        if o['tag'] == ' ' and n['tag'] == ' ':\n",
    "            if len(current_cluster) > 0:\n",
    "                clustered_edits.append(current_cluster)\n",
    "                current_cluster = []\n",
    "            clustered_edits.append([(o, n)])\n",
    "    if len(current_cluster) > 0:\n",
    "        clustered_edits.append(current_cluster)\n",
    "    return clustered_edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e1915414-6f01-4a77-91c4-0e0a8b12d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents(idx_i, idx_j, a, c):\n",
    "    \"\"\"Merges two sentences without spacing errors.\"\"\"\n",
    "    si_text = c[idx_i][a]['text']\n",
    "    sj_text = c[idx_j][a]['text']\n",
    "\n",
    "    if isinstance(si_text, (list, tuple)):\n",
    "        output_list = list(si_text)\n",
    "    else:\n",
    "        output_list = [(idx_i, si_text)]\n",
    "    if isinstance(sj_text, (list, tuple)):\n",
    "        output_list += sj_text\n",
    "    else:\n",
    "        output_list.append((idx_j, sj_text))\n",
    "    return output_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0ec211fa-f914-4140-bb29-f0e0e8a9279b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sents_list(t):\n",
    "    t = sorted(t, key=lambda x: x[0])\n",
    "    t = list(map(lambda x: x[1].strip(), t))\n",
    "    t = ' '.join(t)\n",
    "    return ' '.join(t.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "828f122a-8756-497a-86e4-aa8df4e48116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_in_interval(c, idx_i, idx_j, version):\n",
    "    idx_small, idx_large = min([idx_i, idx_j]), max([idx_i, idx_j])\n",
    "    return any(map(lambda idx: c[idx][version]['text'].strip() != '',  range(idx_small+1, idx_large)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "577a8a11-754b-4b18-b590-ffd6f6b80e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(s, cache):\n",
    "    if isinstance(s, str) and s in cache:\n",
    "        return cache[s], cache\n",
    "    if isinstance(s, list):\n",
    "        s = merge_sents_list(s)\n",
    "    s_lemmas = get_lemmas(s)\n",
    "    s_lemmas = filter_stopword_lemmas(s_lemmas)\n",
    "    s_lemmas = filter_punct(s_lemmas)\n",
    "    cache[s] = s_lemmas\n",
    "    return cache[s], cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a606277f-c1a3-4f6c-940e-6467ccfa843b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_subset(s1_lemmas, s2_lemmas, slack=.5):\n",
    "    \"\"\"Checks if the second sentence is nearly a subset of the first, with up to `slack` words different.\"\"\"\n",
    "    ### get all text (might be a list).\n",
    "    if len(s2_lemmas) > len(s1_lemmas):\n",
    "        return False\n",
    "    if len(s2_lemmas) > 50:\n",
    "        return False\n",
    "    ### check match.\n",
    "    matches = sum(map(lambda word: word in s1_lemmas, s2_lemmas))\n",
    "    return matches >= (len(s2_lemmas) * (1 - slack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef6b5ab-7302-4812-b200-20438685e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_text_spots(c, old_spot_idx, new_spot_idx, version):\n",
    "    ## swap text\n",
    "    text_old = c[old_spot_idx][version]['text']\n",
    "    text_new = c[new_spot_idx][version]['text']\n",
    "    c[new_spot_idx][version]['text'] = text_old\n",
    "    c[old_spot_idx][version]['text'] = text_new\n",
    "    ## swap tags\n",
    "    tag_new = c[new_spot_idx][version]['tag']\n",
    "    tag_old = c[old_spot_idx][version]['tag']\n",
    "    c[new_spot_idx][version]['tag'] = tag_old\n",
    "    c[old_spot_idx][version]['tag'] = tag_new\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f92ec9bf-6619-4e14-9e33-d8f8a9a71dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def merge_cluster(c, slack=.5):\n",
    "    c = list(filter(lambda x: x[0]['text'] != '' or x[1]['text'] != '', c))\n",
    "    old_c = copy.deepcopy(c)\n",
    "    r_c = range(len(c))\n",
    "    keep_going = True\n",
    "    loop_idx = 0\n",
    "    cache = {}\n",
    "\n",
    "    while keep_going:\n",
    "        for active_version in [0, 1]:\n",
    "            inactive_version = abs(active_version - 1)\n",
    "            for idx_i, idx_j in itertools.product(r_c, r_c):\n",
    "                # [(0, 0), (0, 1), (1, 0), (1, 1)]\n",
    "                idx_i, idx_j = (idx_i, idx_j) if active_version == 0 else (idx_j, idx_i)\n",
    "                if (\n",
    "                        (idx_i != idx_j)\n",
    "                        and (c[idx_j][active_version]['text'] != '')\n",
    "                        # and (c[idx_j][inactive_version]['text'] == '')\n",
    "                        and (c[idx_i][inactive_version]['text'] != '')\n",
    "                ):\n",
    "\n",
    "                    # print('active: %s, idx_i: %s, idx_j: %s' % (active_version, idx_i, idx_j))\n",
    "                    s1_lemmas, cache = lemmatize_sentence(c[idx_i][inactive_version]['text'], cache)\n",
    "                    s2_lemmas, cache = lemmatize_sentence(c[idx_j][active_version]['text'], cache)\n",
    "                    if check_subset(s1_lemmas, s2_lemmas, slack=slack):\n",
    "                        # if there's a match, first check:\n",
    "                        combined_text_active = merge_sents(idx_i, idx_j, active_version, c)\n",
    "                        combined_text_inactive = merge_sents(idx_i, idx_j, inactive_version, c)\n",
    "                        c[idx_j][active_version]['text'] = combined_text_active\n",
    "                        c[idx_i][active_version]['text'] = ''\n",
    "                        c[idx_i][inactive_version]['text'] = combined_text_inactive\n",
    "                        c[idx_j][inactive_version]['text'] = ''\n",
    "                        # print('FOUND')\n",
    "                        # print(c)\n",
    "                        # print('active: %s, idx_i: %s, idx_j: %s' % (active_version, idx_i, idx_j))\n",
    "\n",
    "                        #    1. if the two idx's are adjacent, then move the active.\n",
    "                        if abs(idx_i - idx_j) == 1:\n",
    "                            # print('1.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        #    2. if there's both >=1 active AND >=1 inactive in between, don't do anything.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, active_version) and text_in_interval(c, idx_i, idx_j, inactive_version):\n",
    "                            # print('2.')\n",
    "                            pass\n",
    "\n",
    "                        #    3. if there's text in the active version between the two idx's, move the inactive.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, active_version):\n",
    "                            # print('3.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_j, old_spot_idx=idx_i, version=inactive_version)\n",
    "\n",
    "                        #    4. if there's text in the inactive in between the two idx's, move the active.\n",
    "                        elif text_in_interval(c, idx_i, idx_j, inactive_version):\n",
    "                            # print('4.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        #   5. if there's no text inbetween the idx's in either the active or the inactive, move the active.\n",
    "                        elif not (\n",
    "                                text_in_interval(c, idx_i, idx_j, active_version) and\n",
    "                                text_in_interval(c, idx_i, idx_j, inactive_version)\n",
    "                        ):\n",
    "                            # print('5.')\n",
    "                            c = swap_text_spots(c, new_spot_idx=idx_i, old_spot_idx=idx_j, version=active_version)\n",
    "\n",
    "                        ## merge list/text\n",
    "                        for idx, version in itertools.product([idx_i, idx_j], [active_version, inactive_version]):\n",
    "                            if isinstance(c[idx][version]['text'], list):\n",
    "                                c[idx][version]['text'] = merge_sents_list(c[idx][version]['text'])\n",
    "\n",
    "        ## one more merge for safety\n",
    "        for idx, version in itertools.product(r_c, [active_version, inactive_version]):\n",
    "            if isinstance(c[idx][version]['text'], list):\n",
    "                c[idx][version]['text'] = merge_sents_list(c[idx][version]['text'])\n",
    "\n",
    "        if (c == old_c) or (loop_idx > 10000):\n",
    "            # print('done, idx: %s' % loop_idx)\n",
    "            keep_going = False\n",
    "            loop_idx = 0\n",
    "        else:\n",
    "            loop_idx += 1\n",
    "            # print('one more')\n",
    "            old_c = copy.deepcopy(c)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f14bcb6a-b6a4-4b96-9c91-d3e39c384186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_all_clusters(vo, vn, slack=.5):\n",
    "    clustered_edits = cluster_edits(vo, vn)\n",
    "    output_edits = []\n",
    "    for c in clustered_edits:\n",
    "        if len(c) == 1:\n",
    "            c_i = c[0]\n",
    "            if not (c_i[0]['text'] == '' and c_i[1]['text'] == ''):\n",
    "                output_edits.append(c_i)\n",
    "        else:\n",
    "            c_new = merge_cluster(c, slack=slack)\n",
    "            for c_i in c_new:\n",
    "                if not (c_i[0]['text'] == '' and c_i[1]['text'] == ''):\n",
    "                    output_edits.append(c_i)\n",
    "\n",
    "    if len(output_edits) == 0:\n",
    "        return None, None\n",
    "\n",
    "    return zip(*output_edits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "682c4095-63a9-4392-aa87-b6e1e9a0e607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sentence_diff(a_old, a_new, filter_common_sents=True, merge_clusters=True, slack=.5):\n",
    "    ## split sentences\n",
    "    a_old_sents = split_sents(a_old)\n",
    "    a_new_sents = split_sents(a_new)\n",
    "    if filter_common_sents:\n",
    "        a_old_sents = list(filter(filter_sents, a_old_sents))\n",
    "        a_new_sents = list(filter(filter_sents, a_new_sents))\n",
    "    ## group list\n",
    "    vers_old, vers_new = get_list_diff(a_old_sents, a_new_sents)\n",
    "    ## fix errors/ align sentences\n",
    "    if merge_clusters:\n",
    "        vers_old, vers_new = merge_all_clusters(vers_old, vers_new, slack=slack)\n",
    "    return vers_old, vers_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "62fea3d2-9633-41d2-ac54-86f623c780d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addMark(src):\n",
    "    input_docs = src.replace('.\\n\\n', '.\\n\\n#####')\n",
    "    return input_docs\n",
    "\n",
    "def fixMark(src):\n",
    "    input_docs = src.replace('\\n\\n', '.\\n\\n')\n",
    "    input_docs = input_docs.replace('#####', '.\\n\\n')\n",
    "    return input_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ae996a9f-57b9-412a-82d6-7ad6c2b0cc8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old Version\n",
      "Hugo Rafael Chávez Frías (IPA: [uɰo rafael tʃaβes fɾias]) (born July 28, 1954) is the 53rd and current President of Venezuela. As the leader of the Bolivarian Revolution, Chávez promotes his vision of democratic socialism, Latin American integration, and anti-imperialism.  He is also an ardent critic of neoliberal globalization and  US foreign policy.\n",
      "\n",
      "#####United States Representative Charles Rangel (D) also said in a press release that an attack on Bush is attack on all of us (Americans).\n",
      "\n",
      "New Version\n",
      "Hugo Rafael Chávez Frías (IPA: ) (born July 28, 1954) is the 53rd and current President of Venezuela. As the leader of the \"Bolivarian Revolution,\" Chávez promotes his vision of democratic socialism, Latin American integration, and anti-imperialism.  He is also an ardent critic of neoliberal globalization and  US foreign policy.\n",
      "-----------------\n",
      "\n",
      "After Parsing\n",
      "({'text': 'Hugo Rafael Chávez Frías (IPA: [uɰo rafael tʃaβes fɾias]) (born July 28, 1954) is the 53rd and current President of Venezuela.', 'tag': '-'}, {'text': 'As the leader of the Bolivarian Revolution, Chávez promotes his vision of democratic socialism, Latin American integration, and anti-imperialism.', 'tag': '-'})\n",
      "--\n",
      "({'text': 'Hugo Rafael Chávez Frías (IPA: ) (born July 28, 1954) is the 53rd and current President of Venezuela.', 'tag': ' '}, {'text': 'As the leader of the \"Bolivarian Revolution,\" Chávez promotes his vision of democratic socialism, Latin American integration, and anti-imperialism.', 'tag': '+'})\n"
     ]
    }
   ],
   "source": [
    "# Run (main)\n",
    "old_ver = addMark(train_pt[0]['document'])\n",
    "new_ver = addMark(train_pt[0]['summary'])\n",
    "# vers_old, vers_new = get_list_diff(old_ver, new_ver)\n",
    "# old, new = merge_all_clusters(vers_old, vers_new)\n",
    "old, new = get_sentence_diff(fixMark(old_ver), fixMark(new_ver))\n",
    "\n",
    "\n",
    "print('Old Version')\n",
    "print(old_ver)\n",
    "print('\\nNew Version')\n",
    "print(new_ver)\n",
    "print('-----------------')\n",
    "print('\\nAfter Parsing')\n",
    "print(old)\n",
    "print('--')\n",
    "print(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f52b912-03ba-40a6-ad31-fec4960eb286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Labeling(new):\n",
    "    labeled_data = []\n",
    "    for idx in range(len(new)):\n",
    "        if new[idx]['tag']==' ' and new[idx]['text']!='':\n",
    "            labeled_data.append(' [KEEP] ' + new[idx]['text'] + ' [/KEEP]')\n",
    "        elif new[idx]['tag']=='-' and new[idx]['text']!='':\n",
    "            labeled_data.append(' [RM] '+new[idx]['text']+' [/RM]')\n",
    "        elif new[idx]['tag']=='+' and new[idx]['text']!='': \n",
    "            labeled_data.append(' [ADD] '+new[idx]['text']+' [/ADD]')\n",
    "        elif new[idx]['tag']=='#' and new[idx]['text']!='':\n",
    "            labeled_data.append(' [SUB] '+new[idx]['text']+' [/SUB]')\n",
    "    return ''.join(labeled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "f5fe6d0e-c78e-4446-ad1e-d867197bc045",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Testing Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "92aea086-af57-4dd7-913b-878ca96f2f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' [KEEP] Hugo Rafael Chávez Frías (IPA: ) (born July 28, 1954) is the 53rd and current President of Venezuela. [/KEEP] [ADD] As the leader of the \"Bolivarian Revolution,\" Chávez promotes his vision of democratic socialism, Latin American integration, and anti-imperialism. [/ADD]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define old_ver, new_ver in \n",
    "# old, new = get_sentence_diff(old_ver, new_ver)\n",
    "labeled_new = Labeling(new)\n",
    "labeled_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "39b0315c-4dc2-4c7f-8752-be4275ffd8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_ver = addMark(train_pt[1]['document'])\n",
    "new_ver = addMark(train_pt[1]['summary'])\n",
    "old, new = get_sentence_diff(fixMark(old_ver), fixMark(new_ver))\n",
    "output = Labeling(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65b65e12-5033-4020-bae3-bed11145de9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_all, add_all, sub_all = [], [], []\n",
    "for idx in range(5):\n",
    "    old_ver = addMark(train_pt[idx]['document'])\n",
    "    new_ver = addMark(train_pt[idx]['summary'])\n",
    "    old, new = get_sentence_diff(fixMark(old_ver), fixMark(new_ver))\n",
    "    add, sub, keep = 0, 0, 0\n",
    "    for idd in range(len(new)):\n",
    "        if new[idd]['tag']==' ' and new[idd]['text']!='':\n",
    "            keep+=1\n",
    "        elif new[idd]['tag']=='+' and new[idd]['text']!='': \n",
    "            add+=1\n",
    "        elif new[idd]['tag']=='#' and new[idd]['text']!='':\n",
    "            sub+=1\n",
    "    keep_all.append(keep)\n",
    "    add_all.append(add)\n",
    "    sub_all.append(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4a4e0b-6db4-4902-8228-e06f1aa63056",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_changes(old_doc, new_doc):\n",
    "    new_document = []\n",
    "    old_document = []\n",
    "    new_sentences = []\n",
    "    removed_sentences = []\n",
    "\n",
    "    same_sentences = []\n",
    "    changed_sentence_pairs = []\n",
    "\n",
    "    for s_idx, (s_old, s_new) in enumerate(zip(old_doc, new_doc)):\n",
    "        ###\n",
    "        if s_old['text'].strip() != '':\n",
    "            old_document.append(s_old['text'])\n",
    "        if s_new['text'].strip() != '':\n",
    "            new_document.append(s_new['text'])\n",
    "\n",
    "        ###\n",
    "        if s_old['tag'] == '-' and s_new['tag'] == '+':\n",
    "            changed_sentence_pairs.append((s_idx, (s_old['text'], s_new['text'])))\n",
    "\n",
    "        ###\n",
    "        if s_old['tag'] == ' ' and s_new['tag'] == '+':\n",
    "            new_sentences.append(s_new['text'])\n",
    "\n",
    "        ###\n",
    "        if s_new['tag'] == ' ' and s_old['tag'] == '-':\n",
    "            removed_sentences.append(s_old['text'])\n",
    "\n",
    "        if s_new['tag'] == ' ' and s_old['tag'] == ' ':\n",
    "            same_sentences.append(s_old['text'])\n",
    "\n",
    "    return {\n",
    "        'docs': {'old_doc': old_document,\n",
    "                 'new_doc': new_document,\n",
    "                 },\n",
    "        'sentences': {'added_sents': new_sentences,\n",
    "                      'removed_sents': removed_sentences,\n",
    "                      'changed_sent_pairs': changed_sentence_pairs\n",
    "                      }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545be0c0-20d6-49f9-ac46-780433b6a92c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-venv",
   "language": "python",
   "name": "data-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
